{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark \n",
    "> [Main Table of Contents](../../README.md)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In This Notebook\n",
    "\n",
    "- What is PySpark?\n",
    "- Workflow\n",
    "- SQL API\n",
    "    - Example: Builder pattern to create SparkSession\n",
    "- Vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PySpark?\n",
    "\n",
    "- Distributed computing on clusters for large scale parallel data processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "1. Create connection with a cluster with `pyspark.sql.SparkContext(conf=conf)` where the connection is configured through `conf=pyspark.SparkConf()`\n",
    "2. Access PySpark.DataFrame API through instance of `pyspark.sql.SparkSession()`\n",
    "    - This instance is easier-to-use high-level abstraction to RDD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL API\n",
    "\n",
    "Classes | Explanation\n",
    "--- | ---\n",
    "`pyspark.SparkConf()` | Configuration about SparkContext application\n",
    "`pyspark.sql.SparkContext(conf=conf)` | Connection to a cluster<br>Tell Spark how to access a cluster how to access a cluster using `conf` kwarg\n",
    "`pyspark.sql.SparkSession(spark_context)` |  Interface to a cluster<br>The entry point to programming Spark with the Dataset and *_DataFrame API_*<br><br>SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables<br>Create a `SparkSession` using builder pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Example: Builder pattern to create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Word Count\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Term | Explanation\n",
    "--- | ---\n",
    "Cluster | Collection of processing units<br>e.g. Group of separate computers\n",
    "Node | One unit in a cluster\n",
    "Master | Main unit in a cluster responsible for splitting data and distributing to workers\n",
    "Worker | Other units in a cluster that work on segments of data delegated by the master\n",
    "RDD<br>Resilient Distrubuted Datasets|The core datastructure in Spark<br>A fault-tolerant collection of elements that can be operated on in parallel<br>RDD is a low-level API and difficult to use. Instead use the higher abstraction `SparkSession.DataFrame`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
