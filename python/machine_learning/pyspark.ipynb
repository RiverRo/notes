{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark \n",
    "> [Main Table of Contents](../../README.md)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In This Notebook\n",
    "\n",
    "- What is PySpark?\n",
    "- PySpark ML module\n",
    "    - One-hot vector / one-hot encoding\n",
    "- PySpark SQL Module\n",
    "    - Example: Builder pattern to create SparkSession\n",
    "    - `pyspark.sql...` Methods\n",
    "    - `pyspark.sql.DataFrame` Methods\n",
    "    - `pyspark.sql.SparkSession` Methods\n",
    "    - `pyspark.sql.SparkSession.catalog` Methods\n",
    "- Vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PySpark?\n",
    "\n",
    "- Distributed computing on clusters for large scale parallel data processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark ML module (DataFrame-based) [API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html)\n",
    "- `pyspark.ml` only handles numeric data\n",
    "- `pyspark.ml.Pipeline` is the a class that combines all the Estimators and Transformers.\n",
    "    - Reuse the same modeling process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline workflow for ML\n",
    "1. Make sure the values of the columns of interest are in numerical format\n",
    "    - See one-hot vector for converting categorical text data into numerical binary values\n",
    "2. Assemble a vector\n",
    "    - The last step in the Pipeline is to combine all of the columns containing our features of interest into a single column. This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form. You can do this by storing each of the values from a column as an entry in a vector. Then, from the model's point of view, every observation is a vector that contains all of the information about it and a label that tells the modeler what value that observation corresponds to.\n",
    "    - Because of this, the pyspark.ml.feature submodule contains a class called `VectorAssembler`. This Transformer takes all of the columns you specify and combines them into a new vector column\n",
    "3. `pyspark.ml.Pipeline(stages=[<list all estimators, transformers])` creates a reproducible machine learning pipeline\n",
    "    - A simple pipeline, which acts as an estimator. A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer. When Pipeline.fit() is called, the stages are executed in order. If a stage is an Estimator, its Estimator.fit() method will be called on the input dataset to fit a model. Then the model, which is a transformer, will be used to transform the dataset as the input to the next stage. If a stage is a Transformer, its Transformer.transform() method will be called to produce the dataset for the next stage. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages. If stages is an empty list, the pipeline acts as an identity transformer\n",
    "4. Send the dataset through the pipeline by calling `fit` and `transform` with the dataset\n",
    "5. Now data is fully cleaned. Split into train/test sets.\n",
    "    - In Spark it's important to make sure you split the data *AFTER* all the transformations. This is because operations like StringIndexer don't always produce the same index even when given the same list of strings\n",
    "        - Data splitting options\n",
    "            - `df.randomSplit([.6, .4])` means 60% train / 40% test\n",
    "            - `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# EXAMPLE: Full pipeline build\n",
    "# Setup:\n",
    "#   All columns of following df are string type\n",
    "#   Two Spark DataFrames: 1. planes.columns ==> ['tailnum', 'year', 'type', 'manufacturer', 'model', 'engines', 'seats', 'speed', 'engine']\n",
    "#                         2. flights.columns ==> ['year', 'month', 'day', 'dep_time', 'dep_delay', 'arr_time', 'arr_delay', 'carrier', \n",
    "#                                                 'tailnum', 'flight', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute']\n",
    "\n",
    "################################\n",
    "# CLEANING DATA - Mosty manual #\n",
    "################################\n",
    "# Easily cast numerical valued strings to \"integers\" or \"doubles\"\n",
    "# Rename year column so won't have same named columns in final df\n",
    "planes = planes.withColumnRenamed('year', 'plane_year')\n",
    "# Join the DataFrames\n",
    "model_data = flights.join(planes, on='tailnum', how=\"leftouter\")\n",
    "# Cast the columns to integers. Replace exiting columns\n",
    "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast('integer'))\n",
    "model_data = model_data.withColumn(\"air_time\", model_data.air_time.cast('integer'))\n",
    "model_data = model_data.withColumn(\"month\", model_data.month.cast('integer'))\n",
    "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast('integer'))\n",
    "# Create new column \"plane_age\"\n",
    "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)\n",
    "# Assume this dataset will be used by a model that predicts a yes/no question: is the flight late?\n",
    "# Create new column is_late with \"true\"/\"false\" values\n",
    "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n",
    "# Convert to an integer b/c Spark ML can only handle numeric values\n",
    "model_data = model_data.withColumn(\"label\", model_data.is_late.cast('integer'))  # \"label\" default name for the response variable in Spark's ML routines\n",
    "# Remove missing values\n",
    "model_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")\n",
    "\n",
    "####################################################\n",
    "# CLEANING DATA - One-Hot Vector - Create Pipeline #\n",
    "####################################################\n",
    "# flights.dest and flights.carriers are categorical text values, use one-hot vector (one-hot encoding) process, which is a two step process in pyspark\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "carr_indexer = StringIndexer(inputCol='carrier', outputCol='carrier_index')  # maps a string column of labels to an ML column of label indices\n",
    "carr_encoder = OneHotEncoder(inputCol='carrier_index', outputCol='carrier_fact') # maps columns of category indices to binary vectors\n",
    "dest_indexer = StringIndexer(inputCol='dest', outputCol='dest_index')\n",
    "dest_encoder = OneHotEncoder(inputCol='dest_index', outputCol='dest_fact')\n",
    "\n",
    "# MERGE MULTIPLE FEATURE COLUMNS INTO SINGLE VECTOR COLUMN #\n",
    "############################################################\n",
    "# Make a VectorAssembler - feature transformer that merges multiple columns into a vector column\n",
    "# The last step in the Pipeline is to combine all of the columns containing our features into a single column. \n",
    "# This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form\n",
    "# Want model to use following features: month, air_time, plane_age, carrier_fact, dest_fact (last two were new columns created by one-hot vector process)\n",
    "vec_assembler = VectorAssembler(inputCols=['month', 'air_time', 'plane_age', 'carrier_fact', 'dest_fact'], outputCol='features')\n",
    "\n",
    "# CREATE THE ONE-HOT VECTOR PIPELINE #\n",
    "######################################\n",
    "one_hot_vec_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])\n",
    "\n",
    "# USE THE ONE-HOT VECTOR PIPELINE TO FURTHER CLEAN/MODIFY THE DATA #\n",
    "####################################################################\n",
    "final_cleaned_data = one_hot_vec_pipe.fit(model_data).transform(model_data)\n",
    "\n",
    "# SPLIT THE FINAL DATA AND IT IS READY TO USE ON ANY ML MODEL#\n",
    "##############################################################\n",
    "training, test = final_cleaned_data.randomSplit([.6, .4])  # 60% train / 40% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# EXAMPLE: Create and use a logistic regression classification model (Uses Non-linear boundary that determines 0 or 1)\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "###########################################################################################\n",
    "# MODEL OF INTEREST, BUT WOULD LIKE TO OPTIMIZE PARAMETERS TO FIND BEST MODEL FOR DATASET #\n",
    "###########################################################################################\n",
    "# Estimator. Will optimize hyperparams \"regParam\" and \"elasticNetParam\" in this example\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# FIND BEST LOGISTIC REGRESSION CLASSIFICATION MODEL IOW: BEST HYPERPARAMETERS #\n",
    "################################################################################\n",
    "import numpy as np\n",
    "import pyspark.ml.evaluation as evals\n",
    "import pyspark.ml.tuning as tune\n",
    "# Use k-fold cross validation for hyperparameter optimization (find best model process)\n",
    "# `pyspark.ml.tuning.CrossValidator` takes three parameters:  estimator, estimatorParamMaps, evaluator\n",
    "\n",
    "# For the `pyspark.ml.tuning.CrossValidator` evaluator parameter\n",
    "# When doing cross validation for model selection, need a way to compare different models\n",
    "# This model is a binary classification model, use `BinaryClassificationEvaluator` from `pyspark.ml.evaluation`\n",
    "# This evaluator calculates the area under the ROC. \n",
    "# This is a metric that combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number.\n",
    "# Common metric for binary classification algorithms call the AUC, or area under the curve. In this case, the curve is the ROC, or receiver operating curve\n",
    "# The details of what these things actually measure isn't important for this course. The closer the AUC is to 1, the better the model\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "\n",
    "# For the `pyspark.ml.tuning.CrossValidator` estimatorParamMaps parameter\n",
    "# Create a grid of values to search over when looking for the optimal hyperparameters\n",
    "grid = tune.ParamGridBuilder()\n",
    "# Add the hyperparameter to optimize and list of values to try\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "grid = grid.build()\n",
    "\n",
    "cv = tune.CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "# Get all models using cross validation. Computationally very expensive.\n",
    "models = cv.fit(training)  # training data is from previous example above\n",
    "best_lr = models.bestModel # Best model hyperparameters are: regParam=0, elasticNetParam=0, which is default values for lr\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "# WITH BEST MODEL HYPERPARAMETERS CREATE THE MODEL WITH THOSE VALUE #\n",
    "#####################################################################\n",
    "best_lr = LogisticRegression() # Best model hyperparams were defaults, so can call without adjustments\n",
    "best_lr = best_lr.fit(training)\n",
    "\n",
    "\n",
    "##########################################\n",
    "# USE THE MODEL AND EVALUATE PERFORMANCE #\n",
    "##########################################\n",
    "# Use the model to predict on the test set\n",
    "test_results = best_lr.transform(test)  # test data is from the previous example above\n",
    "# Evaluate the predictions - compute the AUC\n",
    "print(evaluator.evaluate(test_results)) # ==> 0.7123313100891033"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot vector (aka one-hot encoding)\n",
    "\n",
    "- Convert categorical text values to numerical binary values\n",
    "- `inputCol` here is the name of the column to index or encode\n",
    "- `outputcol` is the name of the new column to be filled with the result\n",
    "\n",
    "1. Create `StringIndexer(inputCol, outputCol)` (estimator)\n",
    "    - Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column  \n",
    "    \n",
    "2. Encode this numeric column as one-hot vector with `OneHotEncoder(inputCol, outputCol)`\n",
    "    - This works exactly the same way as the StringIndexer by creating an Estimator and then a Transformer. The end result is a column that encodes your categorical feature as a vector that's suitable for machine learning routines (numerical binary values in this case)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark SQL Module\n",
    "\n",
    "1. Create connection with a cluster with `sc = pyspark.sql.SparkContext(conf=conf)` where the connection is configured through `conf=pyspark.SparkConf()`\n",
    "    - `print(sc)` Verify SparkContext is in my environment\n",
    "    - `print(sc.version)` \n",
    "2. Access PySpark.DataFrame API through instance of `pyspark.sql.SparkSession()` from my instance of `SparkContext()`\n",
    "    - This instance is easier-to-use high-level abstraction to RDD\n",
    "3. Use `pyspark.sql.read.<method>` or `pyspark.sql.write.<method>`\n",
    "    - Read/Write from/to external storage (e.g. csv, json, key-value store, etc) directly into and out of pyspark without converting to pandas.DataFrame\n",
    "4. Use `pyspark.sql.SparkSession` and `pyspark.sql.DataFrame` methods for table and spark DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkContext, SparkSession\n",
    "\n",
    "# Example: Build SparkContext and print version to verify SparkContext is in my environment\n",
    "sc = SparkContext(conf=SparkConf())\n",
    "print(sc.version)\n",
    "\n",
    "# Example: Builder pattern to create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Word Count\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()  # Creating multiple SparkSessions and SparkContexts can cause issues, so it's best practice to use the SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pyspark.sql...` API [All](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html)\n",
    "\n",
    "Classes | Explanation\n",
    "--- | ---\n",
    "`pyspark.SparkConf()` | Configuration about SparkContext application\n",
    "`pyspark.sql.SparkContext(conf=conf)` | Connection to a cluster<br>Tell Spark how to access a cluster how to access a cluster using `conf` kwarg\n",
    "`pyspark.sql.SparkSession(spark_context)` |  Interface (DataFrame implementation/API) to a cluster<br>- When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in<br>- SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables<br>- Create a `SparkSession` using builder pattern\n",
    "`pyspark.sql.read.<ioMethod>`<br>e.g. `pyspark.sql.read.csv()` | Load DataFrame from external storage systems (e.g. csv, json, key-value, etc)<br>[I/O methods](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html)<br>[DataFrameReader methods](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html#pyspark.sql.DataFrameReader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pyspark.sql.DataFrame` Methods [All](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html?highlight=show#pyspark.sql.DataFrame.show)\n",
    "\n",
    "- PySpark DataFrames are immutable. Modifications functions will always return *new* DataFrame\n",
    "\n",
    "Method | Explanation\n",
    "--- | ---\n",
    "`pyspark.sql.DataFrame.show(n=20)` | Print the n rows to the console\n",
    "`pyspark.sql.DataFrame.toPandas()` | Convert spark DataFrame to pandas DataFrame\n",
    "`pyspark.sql.DataFrame.createTempView`<br><br>`pyspark.sql.DataFrame.createOrReplaceTempView` | Creates a local temporary view with this DataFrame<br>Throws error if already exists in the catalog<br>Creates or replaces a local temporary view with this DataFrame.<br>The lifetime of both temporary table is tied to the SparkSession that was used to create this DataFrame. \n",
    "`pyspark.sql.DataFrame.groupBy()` | Returns `GroupedData` and can run aggregation functions on them<br>[List of all grouping methods including some agg funcs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/grouping.html)<br>\n",
    "`pyspark.sql.DataFrame.groupBy().agg(*exprs)` | Ability to use other agg functions found under [`pyspark.sql.functions`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html) Aggregate Functions sections with this method\n",
    "`pyspark.sql.DataFrame.select` | Returns new DataFrame with selected columns\n",
    "`pyspark.sql.DataFrame.withColumn()`<br>`pyspark.sql.DataFrame.withColumns()` | Returns new DataFrame by Add new column or Replace existing column of same name\n",
    "`pyspark.sql.DataFrame.withColumnRenamed()` | Returns new DataFrame by renaming existing column of same name\n",
    "`pyspark.sql.DataFrame.join()` | Returns new DataFrame by joining two dfs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pyspark.sql.Column` Methods [All](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/column.html)\n",
    "\n",
    "- Methods that work on dataframe columns\n",
    "\n",
    "Method | Explanation\n",
    "--- | ---\n",
    "`pyspark.sql.Column.cast(\"dtype\")` | SparkML only handles numeric data<br>Popular method to convert datatypes of columns<br>\"dtype\" e.g. \"integer\", \"double\" for decimals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pyspark.sql.SparkSession` Methods [All](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html)\n",
    "\n",
    "Method | Explanation\n",
    "--- | ---\n",
    "`pyspark.sql.SparkSession.catalog` | Interface through which the user may create, drop, alter or query underlying databases, tables, functions etc\n",
    "`pyspark.sql.SparkSession.sql(query)` | Run SQL query on tables in spark cluster<br>Returns `pyspark.sql.dataframe.DataFrame`\n",
    "`pyspark.sql.SparkSession.table(tableName` | Returns table as `pyspark.sql.dataframe.DataFrame`\n",
    "`pyspark.sql.SparkSession.createDataFrame(pd.DataFrame)` | Conver pandas DataFrame to a *locally* *stored* spark DataFrame<br>Locally stored means can use all the Spark DataFrame methods on it, but can't access the data in other contexts<br>e.g. a SQL query (using the .sql() method) that references a locally stored DataFrame will throw an error<br>To access the data in this way, have to save it as a temporary table with `.createTempView()` or `.createOrReplaceTempView()` which registers the DataFrame as a table in the catalog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pyspark.sql.SparkSession.catalog` Methods [All](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/catalog.html)\n",
    "\n",
    "Method | Explanation\n",
    "--- | ---\n",
    "`pyspark.sql.SparkSession.catalog.currentDatabase()`<br>`pyspark.sql.SparkSession.catalog.setCurrentDatabase()` | Returns the current default database in this session<br>Set current default db in the session\n",
    "`pyspark.sql.SparkSession.catalog.listDatabases` | Returns a list of databases available across all sessionsc\n",
    "`pyspark.sql.SparkSession.catalog.listTables(db=None)` | List of all tables/views in the cluster. Uses current db if no db provided.\n",
    "`pyspark.sql.SparkSession.catalog.listFunctions(db=None)` | Returns a list of functions registered in the specified database. API uses current database if no database is provided\n",
    "`pyspark.sql.SparkSession.catalog.listColumns(tableName,db=None)` | Returns a list of columns for the given table/view in the specified database.API uses current database if no database is provided\n",
    "`pyspark.sql.SparkSession.catalog.createTable(tableName, path=None, source=None, schema=None, **options)` | Creates a table based on the dataset in a data source and returns the DataFrame associated with the table\n",
    "`pyspark.sql.SparkSession.catalog.dropGlobalTempView(viewName)` | Drops the global temporary view with the given view name in the catalog. If the view has been cached before, then it will also be uncached. Returns true if this view is dropped successfully, false otherwise.\n",
    "`pyspark.sql.SparkSession.catalog.dropTempView(viewName)` | \tDrops the local temporary view with the given view name in the catalog. If the view has been cached before, then it will also be uncached. Returns true if this view is dropped successfully, false otherwise\n",
    "`pyspark.sql.SparkSession.catalog.isCached(tableName)` | Returns true if the table is currently cached in-memory.\n",
    "`pyspark.sql.SparkSession.catalog.recoverPartitions(tableName)` | \tRecovers all the partitions of the given table and update the catalog. Only works with a partitioned table, and not a view\n",
    "`pyspark.sql.SparkSession.catalog.refreshByPath(path)` | Invalidates and refreshes all the cached data for any DataFrame that contains the given data source path\n",
    "`pyspark.sql.SparkSession.catalog.refreshTable(tableName)` | Invalidates and refreshes all the cached data and metadata of the given table\n",
    "`pyspark.sql.SparkSession.catalog.cacheTable(tableName)` |Caches the specified table in-memory\n",
    "`pyspark.sql.SparkSession.catalog.uncacheTable(tableName)` |Removes the specified table from the in-memory cache\n",
    "`pyspark.sql.SparkSession.catalog.clearCache()` |Removes all cached tables from the in-memory cache."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Term | Explanation\n",
    "--- | ---\n",
    "Cluster | Collection of processing units<br>e.g. Group of separate computers\n",
    "Node | One unit in a cluster\n",
    "Master | Main unit in a cluster responsible for splitting data and distributing to workers\n",
    "Worker | Other units in a cluster that work on segments of data delegated by the master\n",
    "RDD<br>Resilient Distrubuted Datasets|The core datastructure in Spark<br>A fault-tolerant collection of elements that can be operated on in parallel<br>RDD is a low-level API and difficult to use. Instead use the higher abstraction `SparkSession.DataFrame`\n",
    "Estimator | Estimators have `fit` method<br>Estimators that take a DataFrame and output model (transformer)\n",
    "Transformer | Transformers have `transform` method<br>Transformers takes a DataFrame and returns a DataFrame\n",
    "k-fold cross validation | Use Case: Use this for hyperparameter optimization aka: best model selection<br>A way of splitting training/test data and measuring model performance<br>It works by splitting the training data into a few different partitions. The exact number is up to you, but in this course you'll be using PySpark's default value of three. Once the data is split up, one of the partitions is set aside, and the model is fit to the others. Then the error is measured against the held out partition. This is repeated for each of the partitions, so that every block of data is held out and used as a test set exactly once. Then the error on each of the partitions is averaged. This is called the cross validation error of the model, and is a good estimate of the actual error on the held out data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
