{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "- Natural Language Understanding Library\n",
    "- Rule based and machine learning approaches to natural language understanding  \n",
    "- Build rule-based training data sets, use statistical models to predict features in text\n",
    "> [Main Table of Contents](../../../README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In This Notebook\n",
    "- Spacy usage example\n",
    "- Top-Level Functions\n",
    "- Pipelines\n",
    "\t- Tokenizer is a *special* component\n",
    "\t- Stop words\n",
    "\t- Trained pipelines\n",
    "\t- Built-In pipeline components\n",
    "- Cosine Similarity LIMITATION\n",
    "- Containers\n",
    "\t- Language class\n",
    "\t- Doc class\n",
    "\t\t- Doc Attributes\n",
    "\t\t- Doc Methods\n",
    "\t- Span class\n",
    "\t\t- Span Attributes\n",
    "\t\t- Span Methods\n",
    "\t- Token class\n",
    "\t\t- Token/lexical Attributes\n",
    "\t\t- Token/lexical Methods\n",
    "- Statistical Models\n",
    "\t- POS - Parts of Speech\n",
    "\t- Deps - Dependencies\n",
    "\t- Ents - Named Entities\n",
    "- EntityRuler\n",
    "\t- EntityRuler Flow\n",
    "\t- EntityRuler Attributes\n",
    "\t- EntityRuler Methods\n",
    "- Matcher - Pattern Rules\n",
    "\t- Matcher Usage Flow\n",
    "- Phrase Matcher\n",
    "- Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy usage example\n",
    "> Spacy can find elements in a text based on categories (part-of-speech tags, dependency labels, named entity labels, etc) using statistical models like Tagger, DependencyParser, EntityRecognizer, etc or find elements in text based on a rule or pattern, similar to regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
      "('Sebastian Thrun', 'PERSON') ('Google', 'ORG') ('2007', 'DATE') ('American', 'NORP') ('Thrun', 'GPE') "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a trained pipeline\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print((entity.text, entity.label_), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-Level Functions\n",
    "\n",
    "Function/Attribute | Description\n",
    "--- | ---\n",
    "spacy.lang.en.stop_word | List of stop words, used for `Token.is_stop`\n",
    "spacy.load(str) | Load a pipeline using the name of an installed package, string path, Path object<br>Instantiates Language class which is a text-processing pipeline<br>Returns Language object which is conventionally named `nlp`\n",
    "spacy.blank(str) | Create blank pipeline of given language class<br>*Only has language data and tokenization rule<br>`spacy.blank(\"en\")` Equivalent to `English()` from `spacy.lang.en`\n",
    "spacy.lang.en.English() | Create blank English pipeline<br>*Only has language data and tokenization rule<br>Equivalent to `spacy.blank(\"en\")`\n",
    "spacy.explain(str) | Get a description of POS (part of speech) tag, dependency label or entity type<br>See `glossary.py` for full list of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # load a trained pipeline by package name\n",
    "# nlp = spacy.load(\"/path/to/pipeline\") # string path\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\", \"tagger\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "> Text -> Tokenizer -> nlp pipeline/components [Tagger -> Parser -> NER -> ...] -> Doc\n",
    "\n",
    "- nlp pipeline/components are functions and each component receives Doc and returns Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer is a *special* component\n",
    "- While other components receive and return `Doc`, Tokenizer receives text and returns `Doc`\n",
    "- Can only be *one* Tokenizer\n",
    "- Tokenizer *DOES NOT* show up in `nlp.pipe_names`\n",
    "- It is writable and customizer like other components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Pipelines\n",
    "- The three \"en_core_web_\" are english pipelines are optimized for CPU\n",
    "\t- Trained on Genre: written text (blogs, news, comments)\n",
    "\t- Active Pipeline Components: tok2vec, tagger, parser, attribute_ruler, lemmatizer, ner  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tTrained Pipeline Name | Description\n",
    "\t--- | ---\n",
    "\ten_core_web_sm | Doesn't include vectors\n",
    "\ten_core_web_md | Includes 20k unique vectors\n",
    "\ten_core_web_lg | Includes 514k unique vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-In pipeline components\n",
    "\n",
    "Name | Description | Creates\n",
    "--- | --- | ---\n",
    "tagger | part-of-speech (pos) tagger | .tag\n",
    "parse | dependency parser | .dep<br>.head<br>Doc.sents<br>Doc.noun_chunks\n",
    "ner | named entity recognizer | Doc.ents<br>Token.ent_iob<br>Token.ent_type\n",
    "textcat | text classifier | Doc.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity LIMITATION\n",
    "- Doesn't capture complex relationships, synonyms and antonyms\n",
    "\t- Would need word embeddings which capture complex relationships, synonyms/antonyms via heavy deep learning and enormous amount of training data\n",
    "\t- [GH question, can I look up synonyms with spacy](https://github.com/explosion/spaCy/issues/276)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981972483797556\n",
      "0.9740255566391519\n",
      "0.980517596648768\n",
      "0.383055504727787\n",
      "0.5034751138271376\n",
      "0.5143248371438918\n",
      "0.8220816752553904\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc1 = nlp('I am happy')\n",
    "doc2 = nlp('I am joyous')\n",
    "doc3 = nlp('I am sad')\n",
    "print(doc1.similarity(doc2))\n",
    "print(doc1.similarity(doc3))\n",
    "print(doc2.similarity(doc3))\n",
    "word1 = nlp('happy')\n",
    "word2 = nlp('joyous')\n",
    "word3 = nlp('sad') \n",
    "# Expected word1/word2 to have higher similarity than\n",
    "# word1/word3 or word2/word3 but NOT the case!!\n",
    "print(word1.similarity(word2))\n",
    "print(word1.similarity(word3))\n",
    "print(word2.similarity(word3))\n",
    "cat = nlp('cat') \n",
    "dog = nlp('dog')\n",
    "print(cat.similarity(dog)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers\n",
    "- Language\n",
    "- Doc\n",
    "- Span\n",
    "- Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language class\n",
    "- A text-processing pipeline\n",
    "- Conventionally the instantiated object is named `nlp`\n",
    "- A call to `spacy.load`, `spacy.blank`, or `English` returns this object\n",
    "- A pipeline consists of components\n",
    "\t- Components are functions that receive a Doc and returns a Doc  \n",
    "\n",
    "\n",
    "\tAttribute or Method | Description\n",
    "\t--- | ---\n",
    "\tnlp.make_doc | Returns only tokenized doc<br>No other pipeline components<br>TODO: HOW TO TELL IF THIS IS TRUE THOUGH??? SEE EXAMPLE BELOW\n",
    "\tnlp.pipe_names | Returns list of pipeline component names\n",
    "\tnlp.pipeline | Returns list of tuples (component name, component object)\n",
    "\tnlp.pipe(text:str) | Process an iterable of text as stream<br>Returns a generator that yields Doc\n",
    "\tnlp.add_pipe('component_fn_name') | 1. Creates component by the given string name of the component factory<br>2. Adds it to pipeline<br>3. Returns the component<br>Default `last=True`<br>Location kwargs: `before`, `after`, `first`, `last`<br>kwarg `source` add component from another pipeline (Language object)\n",
    "\tnlp.select_pipes(*, disable: str\\|iterable, enable: str\\|iterable) | Context manager<br>kwarg 'disable' Name(s) of pipeline components to disable<br>kwarg 'disable' Name(s) of pipeline components that will not be disabled\n",
    "\t\n",
    "- Class Attributes  \n",
    "\tAttribute | Description\n",
    "\t--- | ---\n",
    "\tLanguage.Defaults | `Defaults` class that contains settings, data and factory method for creating the `np` object and processing pipeline\n",
    "\tLanguage.lang | IETF language tage e.g. 'en' for English\n",
    "\tLanguage.default_config | Base config\n",
    "\n",
    "- Language.Defaults class Attributes  \n",
    "\tAttribute | Description\n",
    "\t--- | ---\n",
    "\tstop_words | List of stop words, used for `Token.is_stop`\n",
    "\ttokenizer_exceptions | Tokenizer exceptions rules, string mapped ot list of token attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use pipe method. Do not use list comprehension to process iterable of texts "
     ]
    }
   ],
   "source": [
    "# nlp.pipe(text:str) EXAMPLE\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "texts = ['Use pipe method.', 'Do not use list comprehension to process iterable of texts']\n",
    "\n",
    "docs = [nlp(doc) for text in texts]  # Bad\n",
    "\n",
    "docs = nlp.pipe(texts)               # Good, docs is generator\n",
    "for doc in docs:\n",
    "    print(doc, end=' ')\n",
    "\n",
    "docs = list(nlp.pipe(texts))          # Good, docs is list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# nlp.select_pipes(*, disable: str\\|iterable, enable: str\\|iterable) EXAMPLE\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "texts = ['Use pipe method.', 'Do not use list comprehension to process iterable of texts']\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"parser\"]):\n",
    "    # Everything in context won't use tagger, parser components\n",
    "    print(nlp.pipe_names)\n",
    "\n",
    "# Everything here will use all pipeline components\n",
    "print(nlp.pipe_names)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: How to TELL IF THIS IS TRUE??  maybe use has_annotation method, how to ck for named entities though?  'ENT' 'ENTS' doesn't work\n",
    "doc = nlp.make_doc('Will only use tokenizer pipeline component') \n",
    "doc.has_annotation('DEP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc class\n",
    "- Container to access linguistic annotations\n",
    "- Access sentences, named entities\n",
    "- A sequence of `Token` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access Doc object\n",
    "\n",
    "# 1. via nlp object\n",
    "doc = nlp('Some text')\n",
    "\n",
    "# 2. via nlp object\n",
    "docs = nlp.pipe(['Hello World', 'Some text']) # generator yields Doc object\n",
    "\n",
    "# 3. Manually instantiate Doc class. Doc(vocab, words, spaces)\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "words = [\"hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc Attributes\n",
    "- Attribute name without trailing underscore usually return numerical version\n",
    "- Attribute name with trailing underscore returns string version\n",
    "\n",
    "\tAttribute | Description\n",
    "\t--- | ---\n",
    "\ttext | String representation of document text\n",
    "\tvocab | The store of lexical types\n",
    "\tlang | Lange of the doc's vocabulary (int)\n",
    "\tlang_ | Lange of the doc's vocabulary (str)\n",
    "\tspans | dictionary of named span groups\n",
    "\tents | Tuple of `Span` objects<br>named entities in the doc\n",
    "\tcats | Typically set by `TextCategorizer`<br>text categories mapped to scores\n",
    "\tsents | Sentences in the doc<br>Sentence spans have no label\n",
    "\tnoun_chunks | If doc syntactically parsed (e.g. DEP model) then attribute contains noun chunks/noun phrase `Span` objects\n",
    "\tsentiment | Scalar value indicating the positivity or negativity of doc (if avail)\n",
    "\tvector | Default: Average of token vectors<br>*Best practice* to use short phrase docs than long docs riddled with common words<br>1D array<br>Numerical representation of doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc Methods  \n",
    "- Methods\n",
    "\n",
    "  \n",
    "\tMethod | Description\n",
    "\t--- | ---\n",
    "\t.has_annotation(attr:int\\|str) | bool<br>Whether doc contains annoation on a `Token` attribute\n",
    "\t.similarity(other) | Default: cosine similarity of word vectors<br>'other' can be Doc, Span, Token, Lexeme objects<br>Higher is more similar<br>Need trained model that contains vectors e.g. `en_core_web_md` or `en_core_web_lg`\n",
    "\t.set_ents(entities:list[Span]) | Set the named entities in the doc\n",
    "\t.set_extension(fn_name) | Add custom attributes on the doc<br>kwarg 'getter' sets a custom doc property<br>kwarg 'method' sets a custom method<br>kwarg 'default' sets a custom writable attribute<br>custom attributes/methods accessible via `doc._.attr_name` or `doc._.method_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parser run on doc? True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Hello World, I am span')\n",
    "# print(f'Named entity model run on doc? {doc.has_annotation(\"ENTS\")}')  # TODO: WHY DOESN'T THIS WORK\n",
    "print(f'Dependency parser run on doc? {doc.has_annotation(\"DEP\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span class\n",
    "- Span is a view of one or more tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am span\n",
      "I am span\n"
     ]
    }
   ],
   "source": [
    "# Access a Span object\n",
    "\n",
    "# 1. Slice bracket Notation on a Doc object. doc[start: exclusive_end]\n",
    "doc = nlp('Hello World, I am span')\n",
    "span = doc[3:]  # ints are token idx NOT char idx\n",
    "print(span)\n",
    "\n",
    "# 2. Manually instantiate Span class. Span(doc, start, exclusive_end)\n",
    "from spacy.tokens import Span\n",
    "span = Span(doc, 3, 6) # ints are token idx NOT char idx\n",
    "print(span)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Span Attributes\n",
    "- Attribute name without trailing underscore usually return numerical version\n",
    "- Attribute name with trailing underscore returns string version\n",
    "\n",
    "\tAttribute | Description\n",
    "\t--- | ---\n",
    "\tdoc | Parent document\n",
    "\ttext | String representation of span text\n",
    "\tsent | The sentence span this span is a part of\n",
    "\tsents | Iterable[Span]\n",
    "\tstart | Token offset for the start of the span\n",
    "\tend | Token offset for the end of the span\n",
    "\tstart_char | Char offset for the start of the span\n",
    "\tend_char | char offset for the end of the span\n",
    "\tents | Tuple of `Span` objects<br>Named entities that fall completely within the span\n",
    "\tnoun_chunks |  Yields `Span`<br>If doc syntactically parsed (e.g. DEP model) then attribute contains noun chunks/noun phrase in the span\n",
    "\tlabel | span's label (int)<br>i.e. named entity type<br>see named entity section\n",
    "\tlabel_ | span's label (str)<br>i.e. named entity type<br>see named entity section\n",
    "\tlemma_ | span's lemma\n",
    "\tsentiment | Scalar value indicating the positivity or negativity of the span\n",
    "\tvector | Default: Average of token vectors<br>*Best practice* to use short phrase spans than long spans riddled with common words<br>1D array<br>Numerical representation of span\n",
    "\troot | Token with the shortest path to the root of the sentence (or the root itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Span Methods  \n",
    "- Methods\n",
    "\n",
    "  \n",
    "\tMethod | Description\n",
    "\t--- | ---\n",
    "\t.as_doc | Create new `Doc` object corresponding to the span, with a copy of the data\n",
    "\t.similarity(other) | Default: cosine similarity of word vectors<br>'other' can be Doc, Span, Token, Lexeme objects<br>Higher is more similar<br>Need trained model that contains vectors e.g. `en_core_web_md` or `en_core_web_lg`\n",
    "\t.set_extension(fn_name) | Add custom attributes on a `Span` object<br>kwarg 'getter' sets a custom doc property<br>kwarg 'method' sets a custom method<br>kwarg 'default' sets a custom writable attribute<br>custom attributes/methods accessible via `span._.attr_name` or `span._.method_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a Token object\n",
    "\n",
    "# 1. Bracket notation on Doc object.  doc[token_idx]\n",
    "doc = nlp('Hello World, I am span')\n",
    "token = doc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token/Lexical Attributes\n",
    "- Attribute name without trailing underscore usually return numerical version\n",
    "- Attribute name with trailing underscore returns string version\n",
    "\n",
    "\tAttributes | Description\n",
    "\t--- | ---\n",
    "\ti | Token index in the parent doc<br>Preferred way to index tokens<br>Instead of e.g. [i for i, token in enumerate(doc)]\n",
    "\tidx | Char index of the token in parent doc\n",
    "\ttext | String representation token content<br>Equivalent to `orth_`\n",
    "\tsent | Sentence *span* this token is a part of\n",
    "\tent_type | named entity type (int)\n",
    "\tent_type_ | named entity type (str)\n",
    "\tpos | part of speech (int)<br>simple UPOS pos tag\n",
    "\tpos_ | part of speech (str)<br>simple UPOS pos tag\n",
    "\ttag | Fine-grained part of speech  (int)<br>detailed pos tag\n",
    "\ttag_ | Fine-grained part of speech  (str)<br>detailed pos tag\n",
    "\tdep | Syntactic dependency relation (int)\n",
    "\tdep_ | Syntactic dependency relation (str)\n",
    "\tlang | Language of parent doc (int)\n",
    "\tlang_ | Language of parent doc (str)\n",
    "\tlemma | Base form of the token, with no inflectional suffixes (int)\n",
    "\tlemma_ | Base form of the token, with no inflectional suffixes (str)\n",
    "\tancestors | Sequence of the token's ancestors\n",
    "\tchildren | Sequence of the token's immediate syntactic children\n",
    "\tsentiment | Scalar value indicating the positivity or negativity of the token\n",
    "\tvector |  1D array<br>Numerical representation of doc\n",
    "\tis_alpha, is_ascii, is_digit, is_lower, is_upper, is_title<br>is_sent_start, is_sent_end, is_space<br>is_punct, is_left_punct, is_right_punct, is_bracket<br>is_quote, is_currency | bool\n",
    "\tlike_url, like_num, like_email | bool\n",
    "\tis_oov | Is the token out-of-vocabulary?<br>Does it not have a word vector?<br>bool\n",
    "\tis_stop | Is token part of a \"stop list\"?<br>bool\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Methods  \n",
    "- Methods\n",
    "\n",
    "  \n",
    "\tMethod | Description\n",
    "\t--- | ---\n",
    "\t.similarity(other) | Default: cosine similarity of word vectors<br>'other' can be Doc, Span, Token, Lexeme objects<br>Higher is more similar<br>Need trained model that contains vectors e.g. `en_core_web_md` or `en_core_web_lg`\n",
    "\t.set_extension(fn_name) | Add custom attributes on a `Token` object<br>kwarg 'getter' sets a custom doc property<br>kwarg 'method' sets a custom method<br>kwarg 'default' sets a custom writable attribute<br>custom attributes/methods accessible via `token._.attr_name` or `token._.method_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Models\n",
    "- Predict features in text after proper training\n",
    "- The accuracy of a statistical models' prediction on new text depends on the training data\n",
    "- Feature Examples\n",
    "\t- e.g. POS model predicts which tag/label most likely applies in any context similar to trained context\n",
    "\t- e.g. NER model predicts which label/ent_type most likely applies in any  context similar to trained context\n",
    "- Statistical Models are context dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS - Parts of Speech\n",
    "- Part Of Speech are part-of-speech catagories\n",
    "\t- e.g. noun, verb, adjective, etc\n",
    "- POS annotations are accessed as `Token` attributes\n",
    "- Useful in word-sense disambiguation\n",
    "\t- e.g. \"The bear is a majestic animal\" vs \"Bear with me\"\n",
    "- Useful in sentiment analysis, question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deps - Dependencies\n",
    "- Dependency Parsers create a syntactice based tree\n",
    "- Provides powerful API to navigate this tree \n",
    "- Dependency annotations are access as `Token` attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous 402 amod\n",
      "cars 429 nsubj\n",
      "shift 8206900633647566924 ROOT\n",
      "insurance 7037928807040764755 compound\n",
      "liability 416 dobj\n",
      "toward 443 prep\n",
      "manufacturers 439 pobj\n",
      "\n",
      "CHUNKS\n",
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n",
      "\n",
      "NAVIGATE TREE\n",
      "Autonomous cars NOUN []\n",
      "cars shift VERB [Autonomous]\n",
      "shift shift VERB [cars, liability, toward]\n",
      "insurance liability NOUN []\n",
      "liability shift VERB [insurance]\n",
      "toward shift VERB [manufacturers]\n",
      "manufacturers toward ADP []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "\n",
    "# dep, dep_ attribute\n",
    "for token in doc:\n",
    "    print(token.text, token.dep, token.dep_)\n",
    "\n",
    "# noun_chunks attribute\n",
    "print('\\nCHUNKS')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)\n",
    "\n",
    "# Navigate parse tree\n",
    "# Use head, children attributes\n",
    "print('\\nNAVIGATE TREE')\n",
    "for token in doc:\n",
    "    print(token.text, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ents - Named Entities Recognition (NER)\n",
    "- Named entities are real-world categories/labels\t\n",
    "- Identifying and classifying named entities into predefined categories\n",
    "- Applications include:\n",
    "\t- Efficient search algorithms\n",
    "\t- Question answering system\n",
    "\t\t- Filter for these categories to answer questions like\n",
    "\t\t- e.g. Who are the people involved in this article?\n",
    "\t\t- e.g. Which countries does this article mention?\n",
    "\t\t- e.g. How much money was exchanged?\n",
    "\t- News article classfication\n",
    "\t- Customer service complaints\n",
    "- Standard way to access named entity annotations is in `doc.ents` property\n",
    "\t- `doc.ents` is a sequence of `Span` objects\n",
    "\t- Entity type can be accessed as a hash value (doc-lvl) `ent.label` \\| `span.label`\n",
    "\t- Entity type can be accessed as a string value (doc-lvl) `ent.label_` \\| `span.label_`\n",
    "\t- Entity type can be accessed as a hash value (tkn-lvl) `token.ent_type`\n",
    "\t- Entity type can be accessed as a string value (tkn-lvl) `token.ent_type_`  \n",
    "- Ents is writable and a `list[Span]`can be set with `doc.set_ents` method\n",
    "- `doc.ents` can be created by `EntityRecognizer` component\n",
    "\t- `EntityRecognizer` components sets `doc.ents`, `token.ent_iob`, `token.ent_type`\n",
    "- When setting `Doc.ents` to create training data, all the spans must be valid and non-overlapping, or an error will be thrown\n",
    "- Spacy has 15+ named entity categories\n",
    "\n",
    "\n",
    "\tBuilt-in entities | Description | Example\n",
    "\t--- | --- | ---\n",
    "\tORG | Organization | Apple\n",
    "\tGPE | Geopolitical entity<br>i.e. countries, cities, states | U.K.\n",
    "\tMONEY | Monetary values<br>including unit | $400 million\n",
    "\tDATE | Date | 2007\n",
    "\tPERSON | Person | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('San Francisco', 'GPE')]\n",
      "[('San', 'GPE'), ('Francisco', 'GPE'), ('considers', ''), ('banning', ''), ('sidewalk', ''), ('delivery', ''), ('robots', '')]\n"
     ]
    }
   ],
   "source": [
    "# ACCESS named entities\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "\n",
    "# token level\n",
    "ents = [(token.text, token.ent_type_) for token in doc]\n",
    "print(ents) # NOTE: Only first two tkns are named entities in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON\n",
      "Mr. Best\n"
     ]
    }
   ],
   "source": [
    "# Set ents with manual construction of sequence[Span]\n",
    "# See EntityRuler for alternate way\n",
    "from spacy.tokens import Span\n",
    "doc = nlp(\"Mr. Best flew to New York on Saturday morning.\")\n",
    "doc.set_ents([Span(doc, 0, 2, \"PERSON\")])\n",
    "print(doc.ents[0].label_)\n",
    "print(doc.ents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EntityRuler\n",
    "- `doc.ents` is writable and can add custom named entities with:\n",
    "\t\tdoc.set_ents([Span(doc, start_tkn_idx, end_tkn_idx, entity_name) for text in doc])\n",
    "- EntityRuler is an alternate way to add spans to `doc.ents`\n",
    "\t- Use token-based rules or\n",
    "\t- Use exact phrase mathces\n",
    "\t- Can be combined with statistical `EntityRecognizer` to boost accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EntityRuler Flow\n",
    "1. Create EntityRuler\n",
    "2. Add to pipeline with `nlp.add_pipe('comp_fn_name')`\n",
    "3. Add patter to ruler with `rule.add_patterns(list[dict])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EntityRuler Attributes\n",
    "\n",
    "Attribute | Description\n",
    "--- | ---\n",
    "labels | All labels present in the match patterns<br>string labels\n",
    "patterns | All patterns added to the entity ruler<br>One dictionary per pattern\n",
    "matcher | Underlying matcher used to process token patterns\n",
    "phrase_matcher | Underlying phrase matcher used to process token patterns\n",
    "token_patterns | Token patterns present in the entity ruler, keyed by label\n",
    "phrase_patterns | Phrase patterns present in the entity ruler, keyed by label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EntityRuler Methods\n",
    "\n",
    "Method | Description\n",
    "--- | ---\n",
    ".add_patterns(list[dict]) | Add patterns to the entity ruler\n",
    ".remove(pattern_id:str) |  Remove pattern by its pattern ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pattern to ruler\n",
    "patterns = [\n",
    "    {\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n",
    "    {\"label\": \"GPE\", \"pattern\": [{\"lower\": \"san\"}, {\"lower\": \"francisco\"}]}\n",
    "]\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher - Pattern Rules\n",
    "- Matchers are beefed up regex to match sequences on tokens\n",
    "- `Matcher` match sequences based on lists of token descriptions\n",
    "- Find words/phrases using rules/patterns describing token attributes\n",
    "\t- Can be token annotations like text, POS tags, lexical attributes\n",
    "- Applying a matcher to a `Doc` gives access to the matched tokens\n",
    "\t- Matched tokens are list[tuples] where (match_id, start_tkn_idx, end_tkn_idx (exclusive))\n",
    "- When writing patterns, one dictionary is one token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matcher Usage Flow\n",
    "1. Initialize Matcher with a vocab\n",
    "2. Add pattern with `matcher.add` method\n",
    "3. Use matcher by calling matcher on a doc\n",
    "4. Returns list[tuples] where (match_id, start, end)\n",
    "\t- match_id is int\n",
    "\t- Use `nlp.vocab.strings[match_id]` to get str version of match_id\n",
    "\t- start, end are token index (where end is exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15578876784678163569 HelloWorld 0 3 Hello, world\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)  # 1. Initialize Matcher with a vocab\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "matcher.add(\"HelloWorld\", [pattern])  # 2. Add pattern to matcher\n",
    "\n",
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)  # 3. Use matcher by calling matcher on a doc\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string rep of match_id\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Matcher\n",
    "- `PhraseMatcher` efficiently match large terminology lists\n",
    "- Accespts match pattersn in the form of `Doc` objects\n",
    "- Same Usage Flow as `Matcher`.\n",
    "\t- Difference is in step 2. Instead of adding pattern list[dict], add list[Doc]\n",
    "- `Doc` pattern can contain single or multiple tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angela Merkel\n",
      "Barack Obama\n",
      "Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "- Training is a supervised learning process which means it cannot guess NEW labels from raw text.\n",
    "- Spacy comes with a range of pre-trained models to predict linguistic annotations, but must almost always fine-tune them with more training data (examples/text and their annotations); train them with more context specific labeled data.\n",
    "\t- Text is the input text the model should predict a label for\n",
    "\t- Label is the label the model should predict\n",
    "- *TRAINING DATA aka EXAMPLES* are created manually by human annotators\n",
    "\t- Semi-automated process\n",
    "\t\t- e.g. spacy's `Matcher`\n",
    "\t- Use Rapid annotations tools\n",
    "\t\t- brat (open-source)\n",
    "\t\t- prodigy ($400) created by same makers as spacy\n",
    "\n",
    "> Some limitations/Pitfalls  \n",
    "\n",
    "Limitation/Pitfall | Description/Solution\n",
    "--- | --- \n",
    "When training a model for new entities/tags/etc. *MUST be careful to not override the old examples the model was trained on*  | Solution is to add old examples with the new examples.\n",
    "Models can't learn everything<br>Models depend on context | Plan label schemes carefully. Generic is better<br>e.g. CLOTHING label better than ADULT_CLOTHING AND CHILD_CLOTHING<br>e.g. TOURIST_DESTINATION is subjective judgement and not a definitive category.  'Paris' and 'Arkansas' ? tourist_destination?  Use GPE (cities, states, countries) generic category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Flow - New Pipeline from Scratch\n",
    "> Training a new category would take a few thousand to million examples\n",
    "\n",
    "1. Start a new pipeline from scratch (only language data and tokenization rule)\n",
    "\t- `spacy.blank('en')` or `spacy.lang.en.English()`\n",
    "2. Create and add a new pipe component\n",
    "3. Initialize the model with random weights\n",
    "\t- `nlp.begin_training()`\n",
    "4. Shuffle and loop over more than once for better accuracy. Train on at few hundred/thousand example\n",
    "5. Predict a few examples with the current weights by calling `nlp.update`\n",
    "6. Compare the predictions with true labels\n",
    "7. Calculate how to change weights to improve predictions\n",
    "8. Update weights slightly\n",
    "9. Go back to Num #5\n",
    "10. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8adb9905b990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'My iPhone11 is acting up again unlike my iphone9'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GADGET\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GADGET\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# text with no entities are important\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'I need a new phone'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m ]\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Start with blank English model (only language and tokenization)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Set up a new pipeline from scratch - NER TRAINING EXAMPLE\n",
    "# NOTE: API changes\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "# Training data = TRAINING_DATA and their annotations\n",
    "TRAINING_DATA = [\n",
    "    ('How to preorder the iPhoneX', {\"entities\": [(20, 28, \"GADGET\")]}),\n",
    "    ('My iPhone11 is acting up again unlike my iphone9', {\"entities\": [(3, 11, \"GADGET\"), (41, 48, \"GADGET\")]}),\n",
    "    # text with no entities are important\n",
    "    ('I need a new phone', {\"entities\", []})  \n",
    "]\n",
    "# Start with blank English model (only language and tokenization)\n",
    "nlp = spacy.blank('en')\n",
    "# Create blank EntityRecognizer, add it to pipeline by providing string name of the factory\n",
    "ner = nlp.add_pipe('ner')\n",
    "# Add a new label\n",
    "ner.add_label('GADGET')\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "# Train for 10 iterations\n",
    "for i in range(10):\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    # Divide samples into batchhes\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        print(batch)\n",
    "        texts = [text for text, annotations in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)  # TODO: <---- API changed\n",
    "# Save the model\n",
    "nlp.to_disk('some_path_to_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Flow - Update an exising model\n",
    "> Updating an existing model would take a few hundred to few thousand examples\n",
    "\n",
    "- Improve the predictions on new data\n",
    "- Improve existing categories, like `PERSON`\n",
    "- Add new categories, just be careful the model doesn't forget the old ones by mixing in old with new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
