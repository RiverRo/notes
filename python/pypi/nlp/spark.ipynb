{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "- Open-source Natural Language Processing Library\n",
    "- Pre-trained models and pipelines\n",
    "- [Finance NLP (released Fall 2022)](https://nlp.johnsnowlabs.com/classify_financial_documents)\n",
    "\t- [Intro to new library](https://medium.com/spark-nlp/spark-nlp-for-finance-is-released-cfa3cc7b9faa)\n",
    "\t- [Spark Tutorials on GH](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings/Public)\n",
    "\t- [Intro to Spark NLP Foundations](https://towardsdatascience.com/introduction-to-spark-nlp-foundations-and-basic-components-part-i-c83b7629ed59)\n",
    "> [Main Table of Contents](../../../README.md)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "- Transfer learning is a means to extract knowledge from a source setting and apply it to a different target setting, and it is a highly effective way to keep improving the accuracy of NLP models and to get reliable accuracies even with small data by leveraging the already existing labelled data of some related task or domain. As a result, there is no need to amass millions of data points in order to train a state-of-the-art model.\n",
    "- The long reign of word vectors as NLP’s core representation technique has seen an exciting new line of challengers such as ELMo, BERT, RoBERTa, ALBERT, XLNet, Ernie, ULMFiT, OpenAI transformer, which are all open-source, including pre-trained models, and can be tuned or reused without a major computing effort. These works made headlines by demonstrating that pre-trained language models can be used to achieve state-of-the-art results on a wide range of NLP tasks, sometimes even surpassing the human level benchmarks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In This Notebook\n",
    "- AnnotatorTypes\n",
    "- Annotators\n",
    "- Transfomers\n",
    "- Pipelines\n",
    "- LightPipeline for small dataset\n",
    "\t- LightPipeline Methods\n",
    "- Finance NLP Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnnotatorTypes\n",
    "\n",
    "AnnotatorType | \n",
    "--- |\n",
    "Document\n",
    "token\n",
    "chunk\n",
    "pos\n",
    "word_embeddings\n",
    "date\n",
    "entity\n",
    "sentiment\n",
    "named_entity\n",
    "dependency\n",
    "labeled_dependency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotators\n",
    "- In Spark NLP, all Annotators are either Estimators or Transformers as we see in Spark ML. An Estimator in Spark ML is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model. A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer that transforms a DataFrame with features into a DataFrame with predictions.\n",
    "\n",
    "\tType of annotator | Description\n",
    "\t--- | ---\n",
    "\tAnnotatorApproach<br>(trainable annotator) |  Extends Estimators from Spark ML, which train on df and produces a model<br>Call `fit()` then `transform()`\n",
    "\tAnnotatorModel<br>(trained annotator) | Extends Transformers which are meant to transform one df into another df through some models<br>Call `transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassifierDLModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9378207d7b02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load ClassiferDL Model trained on bertwiki finance sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentimentClassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifierDLModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"classifierdl_bertwiki_finance_sentiment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence_embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentimentClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ClassifierDLModel' is not defined"
     ]
    }
   ],
   "source": [
    "# load ClassiferDL Model trained on bertwiki finance sentiment\n",
    "sentimentClassifier = ClassifierDLModel.pretrained(\"classifierdl_bertwiki_finance_sentiment\", \"en\").setInputCols([\"sentence_embeddings\"]).setOutputCol(\"class\")\n",
    "sentimentClassifier.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-2-abdf9819f3e5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-abdf9819f3e5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    tokenizer = Tokenizer().setInputCols([“document”]).setOutputCol(“token”)\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer is AnnotatorApproach call fit() then transform()\n",
    "tokenizer = Tokenizer().setInputCols([“document”]).setOutputCol(“token”)\n",
    "tokenizer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer is AnnotatorModel, just call transform()\n",
    "stemmer = Stemmer().setInputCols([“token”]).setOutputCol(“stem”)\n",
    "stemmer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ClassiferDL Model traineed on bertwiki finance sentiment\n",
    "pipeline = PretrainedPipeline('classifierdl_bertwiki_finance_sentiment_pipeline', lang='en')\n",
    "print(pipeline.model.stages)\n",
    "# Seem like this is same as fit and transforming of a manual pipeline created with `Pipeline()` function?\n",
    "# result is dictionary if text_line\n",
    "# result is list of dict if text_list\n",
    "result = pipeline.annotate(text_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "- Transformers used for getting data in or transform teh data from one AnnotatorType to another\n",
    "- Transformers help address the question: \n",
    "\t- What to do if my df doesn't have columns in the types that each Annotator accepts or outputs?\n",
    "- 5 Transformer Types\n",
    "- Transformer do not product model, so just need to call `transform()` on them\n",
    "\n",
    "\n",
    "\tTransformer | Description\n",
    "\t--- | ---\n",
    "\tDocumentAssembler |  To get through the NLP process, we need to get raw data annotated. This is a special transformer that does this for us; it creates the first annotation of type Document which may be used by annotators down the road\n",
    "\tTokenAssembler | Reconstructs a Document type annotation from tokens, usually after these have been normalized, lemmatized, normalized, spell checked, etc, to use this document annotation in further annotators\n",
    "\tDoc2Chunk | Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol\n",
    "\tChunk2Doc | Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result\n",
    "\tFinisher | Once we have our NLP pipeline ready to go, we might want to use our annotation results somewhere else where it is easy to use. The Finisher outputs annotation(s) values into a string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "- Spark pipeline is a sequence of stages (Estimators or Transformers)\n",
    "- The first stage of most pipelines is to create a Spark document through a `DocumentAssembler`\n",
    "- Subsequent stages then take df column(s) with certain AnnotatorType(s) and outputs certain df columns(s) AnnotatorType(s)\n",
    "\t- As the document is passed through pipeline stages, annotations are made to the document\n",
    "- `Pipeline(stages=[...]).fit(train_df)` results in a pipelineModel which can then be called `.transform(test_df)` to make predictions on test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all public pipelines\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "ResourceDownloader.showPublicPipelines(lang=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Methods/Attributes\n",
    "\n",
    "Method/Attribute|\n",
    "--- |\n",
    ".getStages()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightPipeline for small dataset\n",
    "- Distributed processing works best on very large datasets\n",
    "- `Pipeline()`Too much power/bulkiness for small datasets (<=50k sentences), instead use `LightPipeline`\n",
    "\t- Spark NLP LightPipelines are Spark ML pipelines converted into a single machine but the multi-threaded task, becoming more than 10x times faster for smaller amounts of data \n",
    "- Usage:\n",
    "\t- useful working with small datasets, debugging results, or when running either training or prediction from an API that serves one-off requests\n",
    "\t- simply plug in a trained (fitted) pipeline and then annotate a plain text (doesn't have to be df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightPipeline Methods/Attributes\n",
    "\n",
    "Method |\n",
    "---|\n",
    ".annotate()\n",
    ".fullAnnotate()\n",
    ".getStages()\n",
    ".transform()\n",
    ".setIgnoreUnsupported()\n",
    ".getIgnoreUnsupported\n",
    ".pipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import LightPipeline\n",
    "LightPipeline(someTrainedPipeline).annotate(someStringOrArrayDoesNotHaveToBeDF)\n",
    "\n",
    "from sparknlp.base import LightPipeline\n",
    "lightModel = LightPipeline(someTrainedpipelineModel, parse_embeddings=True)\n",
    "lightModel.annotate('Hello there, Nice to finally meet you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Finance NLP (released Fall 2022)](https://nlp.johnsnowlabs.com/classify_financial_documents)\n",
    "\n",
    "Available Features | Description\n",
    "--- | ---\n",
    "sentiment analysis |\n",
    "Financial NER models | Extracting organizations, products, revenue, profit, losses, trading symbols, SEC 10-K information,...\n",
    "text classification | classify texts into specific financial categories\n",
    "Entity linking| to normalize NER entities and link them to public databases/data sources, such as Edgar, Crunchbase, and Nasdaq. By doing that, you can augment Company Names, for example, with externally available data about them\n",
    "Pattern matching |Use context-aware symbolic components to model patterns to combine with Deep Learning information extraction techniques\n",
    "financial embeddings| The result is an n-dimensional embedding matrix, impossible to process by the human eye, containing the interpretation of the text using a financial domain\n",
    "assertion status | infer temporality (present, past, future), probability (possible), or other conditions in the context of the extracted entities\n",
    "relation extraction| infer relations between the extracted entities. For example, the relations of the parties in an agreement\n",
    "question answering | Financial Question Answering model, finetuned on proprietary Financial questions and answers\n",
    "Knowledge graphs | Create Knowledge bases combining entities and relations in a graph, which can be exploited afterward in graph databases\n",
    "Table understanding| Use State-of-the-Art Deep Learning architectures to query tables (extracted, for example, with Spark OCR) with Natural Language. No training is needed\n",
    "Zero-shot-learning | Spark NLP for Finance includes Zero-shot NER and Zero-shot Relation Extraction, to create your information extraction models without any training data, just with examples (prompts)\n",
    "Deidentification | The task of detecting privacy-related entities in text, such as person names, emails, and contact data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
