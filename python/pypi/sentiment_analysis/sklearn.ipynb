{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn\n",
    "- Machine learning toolkit  \n",
    "- Built on Numpy, SciPy, matplotlib\n",
    "> [Main Table of Contents](../../../README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In This Notebook\n",
    "- Stop words\n",
    "- Bag of Words\n",
    "\t- Commonly used parameters in Vectorizer init\n",
    "\t- CountVectorizer\n",
    "\t- TfidfVectorizer\n",
    "- Models\n",
    "\t- Model Selection\n",
    "\t\t- Train Test Splitter\n",
    "\t- Linear Regression\n",
    "\t- Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.  \n",
    "\n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:  \n",
    "\n",
    "- tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "\n",
    "- counting the occurrences of tokens in each document.\n",
    "\n",
    "- normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.  \n",
    "\n",
    "Vectorization the general process of turning a collection of text documents into numerical feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonly used parameters in vectorizer classes\n",
    "\n",
    "Parameter | Description\n",
    "--- | ---\n",
    "token_pattern=regex | Specify regex to tokenize\n",
    "ngram_range=(int, int) | Specify ngram (inclusive)<br>e.g. unigram and bigram<br>ngram_range=(1, 2)\n",
    "max_features=int | Specify max # of features (columns in df)\n",
    "max_df=int\\|float | Ignore terms that have document frequency higher than given threshold<br>If float in range [0.0-1.0] indicates percentage\n",
    "min_df=int\\|float | Ignore terms that have document frequency lower than given threshold<br>AKA 'cut-off' line in literature<br>If float in range [0.0-1.0] indicates percentage\n",
    "stop_words='english'<br>stop_words=ENGLISH_STOP_WORDS | Filter out common and uninformative words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "- Tokenization and occurrence counting\n",
    "- Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "TODO: Add commonly used parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import pandas as pd\n",
    "\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "vectorizer = CountVectorizer()\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "trained = vectorizer.fit(corpus)\n",
    "# Transform documents to document-term matrix\n",
    "transformed = vectorizer.transform(corpus)\n",
    "arr = transformed.toarray()\n",
    "# Get list of feature names (can be used as col names in a pd.df)\n",
    "names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Alt convenience 2-in-1 method for above\n",
    "alt = vectorizer.fit_transform(corpus)\n",
    "# pd.DataFrame(alt.toarray(), columns=alt.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "- Convert a collection of raw documents to a matrix of TF-IDF features \n",
    "- tf-idf(t, d) = tf(t, d) x idf(t) \n",
    "\n",
    "In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.  \n",
    "\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "trained = vectorizer.fit(corpus)\n",
    "# Transform documents to document-term matrix\n",
    "transformed = vectorizer.transform(corpus)\n",
    "arr = transformed.toarray()\n",
    "# Get list of feature names (can be used as col names in a pd.df)\n",
    "names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Alt convenience 2-in-1 method for above\n",
    "alt = vectorizer.fit_transform(corpus).toarray()\n",
    "print(alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "- Train Test Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Splitter\n",
    "- sklearn.model_selection.train_test_split()\n",
    "- Function will randomly split a given dataset (lists, numpy arrays, pd.df, scipy-sparse matrices) in to `X_train, y_train, x_test, y_test`, which can then be used in models' `fit, transform, predict, etc.` methods\n",
    "\n",
    "Parameter | Description\n",
    "--- | ---\n",
    "test_size=int\\|float |If float [0.0-1.0] indicates percentage<br>If int indicates absolute # of test samples\n",
    "train_size=int\\|float |If float [0.0-1.0] indicates percentage<br>If int indicates absolute # of test samples\n",
    "random_state=int | Similar to random seed used for reproducible output\n",
    "shuffle=bool | Shuffly data before splitting\n",
    "stratify=array-like | data is split in stratified fasion using input as class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "- Best fit line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    " - Best fit sigmoid function\n",
    " - Typically used in classification problems (discrete categories)\n",
    "\t- Classification is a form of pattern recognition\n",
    "\t\t- Similar number sequences, words or sentiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
