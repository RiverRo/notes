{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn\n",
    "- Machine learning toolkit  \n",
    "- Built on Numpy, SciPy, matplotlib\n",
    "> [Main Table of Contents](../../../README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In This Notebook\n",
    "- Stop words\n",
    "- Bag of Words\n",
    "\t- LIMITATION OF THIS PROCESS\n",
    "\t- Commonly used parameters in Vectorizer init\n",
    "\t- CountVectorizer\n",
    "\t- TfidfVectorizer\n",
    "- Models\n",
    "\t- Model Selection\n",
    "\t\t- Train Test Splitter\n",
    "\t- Model Accuracy\n",
    "\t- Linear Regression\n",
    "\t- Logistic Regression\n",
    "\t- Naive Bayes MultinomialNB\n",
    "- Cosine Similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.  \n",
    "\n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:  \n",
    "\n",
    "- tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "\n",
    "- counting the occurrences of tokens in each document.\n",
    "\n",
    "- normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.  \n",
    "\n",
    "- BOW is part of the text preprocessing step\n",
    "- BOW shortcomings:  Doesn't take in context. Can use ngram_range to resolve this. Though the increase in ngrams may only do marginally better and the added inefficiency (time and dimensionality) may not be worth it.\n",
    "\t- e.g. \"The movie was good and not boring\" and \"The movie was not good and boring\" has same BOW, but opposite sentiment.  \n",
    "\n",
    "Vectorization the general process of turning a collection of text documents into numerical feature vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIMITATIONS OF BOW AND TFIDF and cosine similarities\n",
    "- Doesn't capture complex relationships, synonyms and antonyms\n",
    "\t- Would need word embeddings which capture complex relationships, synonyms/antonyms via heavy deep learning and enormous amount of training data\n",
    "\t- [GH question, can I look up synonyms with spacy](https://github.com/explosion/spaCy/issues/276)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.25861529 0.25861529]\n",
      " [0.25861529 1.         0.25861529]\n",
      " [0.25861529 0.25861529 1.        ]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = ['I am happy', 'I am joyous', 'I am sad']\n",
    "corpus_two = ['happy', 'joyous', 'sad']\n",
    "vectorizer = TfidfVectorizer()\n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "sim = cosine_similarity(matrix)\n",
    "print(sim)\n",
    "vectorizer_two = TfidfVectorizer()\n",
    "matrix_two = vectorizer_two.fit_transform(corpus_two)\n",
    "sim_two = cosine_similarity(matrix_two)\n",
    "print(sim_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonly used parameters in vectorizer classes\n",
    "\n",
    "Parameter | Description\n",
    "--- | ---\n",
    "token_pattern=regex | Specify regex to tokenize\n",
    "tokenizer | Add custom function<br> Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
    "lowercase=bool | Conver all chars to lower before tokenizing\n",
    "strip_accents='ascii', 'unicode', None | Remove accents and perform other char normalizations\n",
    "stop_words='english', list, None | Remove common, uninformative words<br>e.g. stop_words=ENGLISH_STOP_WORDS\n",
    "ngram_range=(int, int) | Specify ngram (inclusive)<br>e.g. unigram and bigram<br>ngram_range=(1, 2)\n",
    "max_features=int | Specify max # of features (columns in df)\n",
    "max_df=int\\|float | Ignore terms that have document frequency higher than given threshold<br>If float in range [0.0-1.0] indicates percentage\n",
    "min_df=int\\|float | Ignore terms that have document frequency lower than given threshold<br>AKA 'cut-off' line in literature<br>If float in range [0.0-1.0] indicates percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "- Tokenization and occurrence counting of every token\n",
    "- Convert a collection of text documents to a matrix of token counts\n",
    "- Use CountVectorizer to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import pandas as pd\n",
    "\n",
    "# Every token in this corpus is a column, this is why text pre-processing is important, to reduce the number of dimensions by combining similar words together and eliminating unimportant words.  Most of the dimensions (columns) have value 0 since most words don't occur in a particular sentence\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "vectorizer = CountVectorizer()\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "trained = vectorizer.fit(corpus)\n",
    "# Transform documents to document-term matrix\n",
    "transformed = vectorizer.transform(corpus) \n",
    "arr = transformed.toarray()  # use to build pd.df\n",
    "# Get list of feature names (can be used as col names in a pd.df)\n",
    "names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Alt convenience 2-in-1 method for above\n",
    "# when using train/test sets only transform the test set, do not fit\n",
    "alt = vectorizer.fit_transform(corpus)  \n",
    "# Map the column names to the vocabulary\n",
    "# pd.DataFrame(alt.toarray(), columns=alt.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "- Convert a collection of raw documents to a matrix of TF-IDF features \n",
    "- tf-idf(t, d) = tf(t, d) x idf(t) \n",
    "- Higher the tfidf weight, the more important the word is in characterizing a document\n",
    "\t- May imply the word is highly exclusive to that document\n",
    "- Application in:\n",
    "\t- Automatically detect stopwords b/c useful in finding words that characterize a particular document\n",
    "\t- Sometimes better performance in predictive modeling\n",
    "\t- Using TfidfVectorizer with Cosine Similarity or linear_kernel functions are useful in recommendation systems\n",
    "\t- e.g. If a person liked the movie \"Godfather\" then might like other movies with similar plot lines\n",
    "\n",
    "In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.  \n",
    "\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Learn a vocabulary dictionary of all tokens in the raw documents\n",
    "trained = vectorizer.fit(corpus)\n",
    "# Transform documents to document-term matrix\n",
    "transformed = vectorizer.transform(corpus)\n",
    "arr = transformed.toarray()  # use to build pd.df\n",
    "# Get list of feature names (can be used as col names in a pd.df)\n",
    "names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Alt convenience 2-in-1 method for above\n",
    "# when using train/test sets only transform the test set, do not fit\n",
    "alt = vectorizer.fit_transform(corpus).toarray()\n",
    "print(alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "- Train Test Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy\n",
    "- Use `sklearn.metrics.accuracy_score` instead of the built-in model score methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Splitter\n",
    "- sklearn.model_selection.train_test_split()\n",
    "- Function will randomly split a given dataset (lists, numpy arrays, pd.df, scipy-sparse matrices) in to `X_train, y_train, x_test, y_test`, which can then be used in models' `fit, transform, predict, etc.` methods\n",
    "\n",
    "\tParameter | Description\n",
    "\t--- | ---\n",
    "\ttest_size=int\\|float |If float [0.0-1.0] indicates percentage<br>If int indicates absolute # of test samples\n",
    "\ttrain_size=int\\|float |If float [0.0-1.0] indicates percentage<br>If int indicates absolute # of test samples\n",
    "\trandom_state=int | Similar to random seed used for reproducible output\n",
    "\tshuffle=bool | Shuffly data before splitting\n",
    "\tstratify=array-like | data is split in stratified fasion using input as class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "- Best fit line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    " - Best fit sigmoid function\n",
    " - Typically used in classification problems (discrete categories)\n",
    "\t- Classification is a form of pattern recognition\n",
    "\t\t- Similar number sequences, words or sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes MultinomialNB\n",
    "- Naive Bayes *classifier* for multinomial models.\n",
    "- The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is 100.00% accurate\n"
     ]
    }
   ],
   "source": [
    "# Bulid a text classfier EXAMPLE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# note TfidfTransformer not TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer  \n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('../../../data/customer_call_transcriptions.csv', header=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.3)\n",
    "# Create text classifier pipeline\n",
    "text_classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "# Fit thhe classifier pipeline on the training data\n",
    "text_classifier.fit(X_train, y_train)\n",
    "# Make predictions and compare them to test labels\n",
    "predictions = text_classifier.predict(X_test)\n",
    "accuracy = 100 * np.mean(predictions == y_test)\n",
    "print(f'The model is {accuracy:.2f}% accurate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "- Value lies between 0 and 1\n",
    "- Using TfidfVectorizer with Cosine Similarity or linear_kernel functions are useful in recommendation systems\n",
    "\t- e.g. If a person liked the movie \"Godfather\" then might like other movies with similar plot lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
