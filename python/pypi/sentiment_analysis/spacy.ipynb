{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "- Natural Language Understanding Library\n",
    "- Rule based and machine learning approaches to natural language understanding  \n",
    "- Build rule-based training data sets, use statistical models to predict features in text\n",
    "> [Main Table of Contents](../../../README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In This Notebook\n",
    "- Spacy usage example\n",
    "- Top-Level Functions\n",
    "- Pipelines\n",
    "\t- Tokenizer is a *special* component\n",
    "\t- Trained pipelines\n",
    "- Containers\n",
    "\t- Language class\n",
    "\t- Doc class\n",
    "\t\t- Doc Attributes\n",
    "\t\t- Doc Methods\n",
    "\t- Span class\n",
    "\t\t- Span Attributes\n",
    "\t\t- Span Methods\n",
    "\t- Token class\n",
    "\t\t- Token/lexical Attributes\n",
    "\t\t- Token/lexical Methods\n",
    "- Statistical Models\n",
    "\t- POS - Parts of Speech\n",
    "\t- Deps - Dependencies\n",
    "\t- Ents - Named Entities\n",
    "- Matcher - Pattern Rules\n",
    "\t- Matcher Usage Flow\n",
    "- Phrase Matcher\n",
    "- Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy usage example\n",
    "> Spacy can find elements in a text based on categories (part-of-speech tags, dependency labels, named entity labels, etc) using statistical models like Tagger, DependencyParser, EntityRecognizer, etc or find elements in text based on a rule or pattern, similar to regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
      "('Sebastian Thrun', 'PERSON') ('Google', 'ORG') ('2007', 'DATE') ('American', 'NORP') ('Thrun', 'GPE') "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a trained pipeline\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print((entity.text, entity.label_), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-Level Functions\n",
    "\n",
    "Function/Attribute | Description\n",
    "--- | ---\n",
    "spacy.lang.en.stop_word | List of stop words, used for `Token.is_stop`\n",
    "spacy.load(str) | Load a pipeline using the name of an installed package, string path, Path object<br>Instantiates Language class which is a text-processing pipeline<br>Returns Language object which is conventionally named `nlp`\n",
    "spacy.blank(str) | Create blank pipeline of given language class<br>Equivalent to `English()` from `spacy.lang.en`\n",
    "spacy.explain(str) | Get a description of POS (part of speech) tag, dependency label or entity type<br>See `glossary.py` for full list of terms\n",
    "spacy.lang.en.English() | Access to English class<br>Equivalent to `spacy.blank(\"en\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # load a trained pipeline by package name\n",
    "# nlp = spacy.load(\"/path/to/pipeline\") # string path\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\", \"tagger\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "> Text -> Tokenizer -> nlp pipeline/components [Tagger -> Parser -> NER -> ...] -> Doc\n",
    "\n",
    "- nlp pipeline/components are functions and each component receives Doc and returns Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer is a *special* component\n",
    "- While other components receive and return `Doc`, Tokenizer receives text and returns `Doc`\n",
    "- Can only be *one* Tokenizer\n",
    "- Tokenizer *DOES NOT* show up in `nlp.pipe_names`\n",
    "- It is writable and customizer like other components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Pipelines\n",
    "- The three \"en_core_web_\" are english pipelines are optimized for CPU\n",
    "\t- Trained on Genre: written text (blogs, news, comments)\n",
    "\t- Active Pipeline Components: tok2vec, tagger, parser, attribute_ruler, lemmatizer, ner  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tTrained Pipeline Name | Description\n",
    "\t--- | ---\n",
    "\ten_core_web_sm | Doesn't include vectors\n",
    "\ten_core_web_md | Includes 20k unique vectors\n",
    "\ten_core_web_lg | Includes 514k unique vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers\n",
    "- Language\n",
    "- Doc\n",
    "- Span\n",
    "- Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language class\n",
    "- A text-processing pipeline\n",
    "- Conventionally the instantiated object is named `nlp`\n",
    "- A call to `spacy.load`, `spacy.blank`, or `English` returns this object\n",
    "- A pipeline consists of components\n",
    "\t- Components are functions that receive a Doc and returns a Doc  \n",
    "\n",
    "\n",
    "\tAttribute or Method | Description\n",
    "\t--- | ---\n",
    "\tnlp.make_doc | Returns only tokenized doc<br>No other pipeline components<br>TODO: HOW TO TELL IF THIS IS TRUE THOUGH??? SEE EXAMPLE BELOW\n",
    "\tnlp.pipe_names | Returns list of pipeline component names\n",
    "\tnlp.pipeline | Returns list of tuples (component name, component object)\n",
    "\tnlp.pipe(text:str) | Process an iterable of text as stream<br>Returns a generator that yields Doc\n",
    "\tnlp.add_pipe('component_fn_name') | Add any pipeline component<br>Default `last=True`<br>Location kwargs: `before`, `after`, `first`, `last`<br>kwarg `source` add component from another pipeline (Language object)\n",
    "\tnlp.select_pipes(*, disable: str\\|iterable, enable: str\\|iterable) | Context manager<br>kwarg 'disable' Name(s) of pipeline components to disable<br>kwarg 'disable' Name(s) of pipeline components that will not be disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use pipe method. Do not use list comprehension to process iterable of texts "
     ]
    }
   ],
   "source": [
    "# nlp.pipe(text:str) EXAMPLE\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "texts = ['Use pipe method.', 'Do not use list comprehension to process iterable of texts']\n",
    "\n",
    "docs = [nlp(doc) for text in texts]  # Bad\n",
    "\n",
    "docs = nlp.pipe(texts)               # Good, docs is generator\n",
    "for doc in docs:\n",
    "    print(doc, end=' ')\n",
    "\n",
    "docs = list(nlp.pipe(texts))          # Good, docs is list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# nlp.select_pipes(*, disable: str\\|iterable, enable: str\\|iterable) EXAMPLE\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "texts = ['Use pipe method.', 'Do not use list comprehension to process iterable of texts']\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"parser\"]):\n",
    "    # Everything in context won't use tagger, parser components\n",
    "    print(nlp.pipe_names)\n",
    "\n",
    "# Everything here will use all pipeline components\n",
    "print(nlp.pipe_names)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: How to TELL IF THIS IS TRUE??  maybe use has_annotation method, how to ck for named entities though?  'ENT' 'ENTS' doesn't work\n",
    "doc = nlp.make_doc('Will only use tokenizer pipeline component') \n",
    "doc.has_annotation('DEP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc class\n",
    "- Container to access linguistic annotations\n",
    "- Access sentences, named entities\n",
    "- A sequence of `Token` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access Doc object\n",
    "\n",
    "# 1. via nlp object\n",
    "doc = nlp('Some text')\n",
    "\n",
    "# 2. via nlp object\n",
    "docs = nlp.pipe(['Hello World', 'Some text']) # generator yields Doc object\n",
    "\n",
    "# 3. Manually instantiate Doc class. Doc(vocab, words, spaces)\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "words = [\"hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc Attributes\n",
    "- Attribute name without trailing underscore usually return numerical version\n",
    "- Attribute name with trailing underscore returns string version\n",
    "\n",
    "\tAttribute | Description\n",
    "\t--- | ---\n",
    "\ttext | String representation of document text\n",
    "\tvocab | The store of lexical types\n",
    "\tlang | Lange of the doc's vocabulary (int)\n",
    "\tlang_ | Lange of the doc's vocabulary (str)\n",
    "\tspans | dictionary of named span groups\n",
    "\tents | Tuple of `Span` objects<br>named entities in the doc\n",
    "\tcats | Typically set by `TextCategorizer`<br>text categories mapped to scores\n",
    "\tsents | Sentences in the doc<br>Sentence spans have no label\n",
    "\tnoun_chunks | If doc syntactically parsed (e.g. DEP model) then attribute contains noun chunks/noun phrase `Span` objects\n",
    "\tsentiment | Scalar value indicating the positivity or negativity of doc (if avail)\n",
    "\tvector | Default: Average of token vectors<br>*Best practice* to use short phrase docs than long docs riddled with common words<br>1D array<br>Numerical representation of doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc Methods  \n",
    "- Methods\n",
    "\n",
    "  \n",
    "\tMethod | Description\n",
    "\t--- | ---\n",
    "\t.has_annotation(attr:int\\|str) | bool<br>Whether doc contains annoation on a `Token` attribute\n",
    "\t.similarity(other) | Default: cosine similarity of word vectors<br>'other' can be Doc, Span, Token, Lexeme objects<br>Higher is more similar<br>Need trained model that contains vectors e.g. `en_core_web_md` or `en_core_web_lg`\n",
    "\t.set_ents(entities:list[Span]) | Set the named entities in the doc\n",
    "\t.set_extension(fn_name) | Add custom attributes on the doc<br>kwarg 'getter' sets a custom doc property<br>kwarg 'method' sets a custom method<br>kwarg 'default' sets a custom writable attribute<br>custom attributes/methods accessible via `doc._.attr_name` or `doc._.method_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parser run on doc? True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Hello World, I am span')\n",
    "# print(f'Named entity model run on doc? {doc.has_annotation(\"ENTS\")}')  # TODO: WHY DOESN'T THIS WORK\n",
    "print(f'Dependency parser run on doc? {doc.has_annotation(\"DEP\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span class\n",
    "- Span is a view of one or more tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am span\n",
      "I am span\n"
     ]
    }
   ],
   "source": [
    "# Access a Span object\n",
    "\n",
    "# 1. Slice bracket Notation on a Doc object. doc[start: exclusive_end]\n",
    "doc = nlp('Hello World, I am span')\n",
    "span = doc[3:]  # ints are token idx NOT char idx\n",
    "print(span)\n",
    "\n",
    "# 2. Manually instantiate Span class. Span(doc, start, exclusive_end)\n",
    "from spacy.tokens import Span\n",
    "span = Span(doc, 3, 6) # ints are token idx NOT char idx\n",
    "print(span)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Span Attributes\n",
    "- Attribute name without trailing underscore usually return numerical version\n",
    "- Attribute name with trailing underscore returns string version\n",
    "\n",
    "\tAttribute | Description\n",
    "\t--- | ---\n",
    "\tdoc | Parent document\n",
    "\ttext | String representation of span text\n",
    "\tsent | The sentence span this span is a part of\n",
    "\tsents | Iterable[Span]\n",
    "\tstart | Token offset for the start of the span\n",
    "\tend | Token offset for the end of the span\n",
    "\tstart_char | Char offset for the start of the span\n",
    "\tend_char | char offset for the end of the span\n",
    "\tents | Tuple of `Span` objects<br>Named entities that fall completely within the span\n",
    "\tnoun_chunks |  Yields `Span`<br>If doc syntactically parsed (e.g. DEP model) then attribute contains noun chunks/noun phrase in the span\n",
    "\tlabel | span's label (int)<br>i.e. named entity type<br>see named entity section\n",
    "\tlabel_ | span's label (str)<br>i.e. named entity type<br>see named entity section\n",
    "\tlemma_ | span's lemma\n",
    "\tsentiment | Scalar value indicating the positivity or negativity of the span\n",
    "\tvector | Default: Average of token vectors<br>*Best practice* to use short phrase spans than long spans riddled with common words<br>1D array<br>Numerical representation of span\n",
    "\troot | Token with the shortest path to the root of the sentence (or the root itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Span Methods  \n",
    "- Methods\n",
    "\n",
    "  \n",
    "\tMethod | Description\n",
    "\t--- | ---\n",
    "\t.as_doc | Create new `Doc` object corresponding to the span, with a copy of the data\n",
    "\t.similarity(other) | Default: cosine similarity of word vectors<br>'other' can be Doc, Span, Token, Lexeme objects<br>Higher is more similar<br>Need trained model that contains vectors e.g. `en_core_web_md` or `en_core_web_lg`\n",
    "\t.set_extension(fn_name) | Add custom attributes on a `Span` object<br>kwarg 'getter' sets a custom doc property<br>kwarg 'method' sets a custom method<br>kwarg 'default' sets a custom writable attribute<br>custom attributes/methods accessible via `span._.attr_name` or `span._.method_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a Token object\n",
    "\n",
    "# 1. Bracket notation on Doc object.  doc[token_idx]\n",
    "doc = nlp('Hello World, I am span')\n",
    "token = doc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token/Lexical Attributes\n",
    "- Attribute name without trailing underscore usually return numerical version\n",
    "- Attribute name with trailing underscore returns string version\n",
    "\n",
    "\tAttributes | Description\n",
    "\t--- | ---\n",
    "\ti | Token index in the parent doc<br>Preferred way to index tokens<br>Instead of e.g. [i for i, token in enumerate(doc)]\n",
    "\tidx | Char index of the token in parent doc\n",
    "\ttext | String representation token content<br>Equivalent to `orth_`\n",
    "\tsent | Sentence *span* this token is a part of\n",
    "\tent_type | named entity type (int)\n",
    "\tent_type_ | named entity type (str)\n",
    "\tpos | part of speech (int)<br>simple UPOS pos tag\n",
    "\tpos_ | part of speech (str)<br>simple UPOS pos tag\n",
    "\ttag | Fine-grained part of speech  (int)<br>detailed pos tag\n",
    "\ttag_ | Fine-grained part of speech  (str)<br>detailed pos tag\n",
    "\tdep | Syntactic dependency relation (int)\n",
    "\tdep_ | Syntactic dependency relation (str)\n",
    "\tlang | Language of parent doc (int)\n",
    "\tlang_ | Language of parent doc (str)\n",
    "\tlemma | Base form of the token, with no inflectional suffixes (int)\n",
    "\tlemma_ | Base form of the token, with no inflectional suffixes (str)\n",
    "\tancestors | Sequence of the token's ancestors\n",
    "\tchildren | Sequence of the token's immediate syntactic children\n",
    "\tsentiment | Scalar value indicating the positivity or negativity of the token\n",
    "\tvector |  1D array<br>Numerical representation of doc\n",
    "\tis_alpha, is_ascii, is_digit, is_lower, is_upper, is_title<br>is_sent_start, is_sent_end, is_space<br>is_punct, is_left_punct, is_right_punct, is_bracket<br>is_quote, is_currency | bool\n",
    "\tlike_url, like_num, like_email | bool\n",
    "\tis_oov | Is the token out-of-vocabulary?<br>Does it not have a word vector?<br>bool\n",
    "\tis_stop | Is token part of a \"stop list\"?<br>bool\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Methods  \n",
    "- Methods\n",
    "\n",
    "  \n",
    "\tMethod | Description\n",
    "\t--- | ---\n",
    "\t.similarity(other) | Default: cosine similarity of word vectors<br>'other' can be Doc, Span, Token, Lexeme objects<br>Higher is more similar<br>Need trained model that contains vectors e.g. `en_core_web_md` or `en_core_web_lg`\n",
    "\t.set_extension(fn_name) | Add custom attributes on a `Token` object<br>kwarg 'getter' sets a custom doc property<br>kwarg 'method' sets a custom method<br>kwarg 'default' sets a custom writable attribute<br>custom attributes/methods accessible via `token._.attr_name` or `token._.method_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Models\n",
    "- Predict features in text after proper training\n",
    "- Feature Examples\n",
    "\t- e.g. POS model predicts which tag/label most likely applies in any context similar to trained context\n",
    "\t- e.g. NER model predicts which label/ent_type most likely applies in any  context similar to trained context\n",
    "- Statistical Models are context dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS - Parts of Speech\n",
    "- Part Of Speech are part-of-speech catagories\n",
    "\t- e.g. noun, verb, adjective, etc\n",
    "- POS annotations are accessed as `Token` attributes\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deps - Dependencies\n",
    "- Dependency Parsers create a syntactice based tree\n",
    "- Provides powerful API to navigate this tree \n",
    "- Dependency annotations are access as `Token` attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous 402 amod\n",
      "cars 429 nsubj\n",
      "shift 8206900633647566924 ROOT\n",
      "insurance 7037928807040764755 compound\n",
      "liability 416 dobj\n",
      "toward 443 prep\n",
      "manufacturers 439 pobj\n",
      "\n",
      "CHUNKS\n",
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n",
      "\n",
      "NAVIGATE TREE\n",
      "Autonomous cars NOUN []\n",
      "cars shift VERB [Autonomous]\n",
      "shift shift VERB [cars, liability, toward]\n",
      "insurance liability NOUN []\n",
      "liability shift VERB [insurance]\n",
      "toward shift VERB [manufacturers]\n",
      "manufacturers toward ADP []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "\n",
    "# dep, dep_ attribute\n",
    "for token in doc:\n",
    "    print(token.text, token.dep, token.dep_)\n",
    "\n",
    "# noun_chunks attribute\n",
    "print('\\nCHUNKS')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)\n",
    "\n",
    "# Navigate parse tree\n",
    "# Use head, children attributes\n",
    "print('\\nNAVIGATE TREE')\n",
    "for token in doc:\n",
    "    print(token.text, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ents - Named Entities\n",
    "- Named entities are real-world categories/labels\t\n",
    "- Standard way to access named entity annotations is in `doc.ents` property\n",
    "\t- `doc.ents` is a sequence of `Span` objects\n",
    "\t- Entity type can be accessed as a hash value (doc-lvl) `ent.label` \\| `span.label`\n",
    "\t- Entity type can be accessed as a string value (doc-lvl) `ent.label_` \\| `span.label_`\n",
    "\t- Entity type can be accessed as a hash value (tkn-lvl) `token.ent_type`\n",
    "\t- Entity type can be accessed as a string value (tkn-lvl) `token.ent_type_`  \n",
    "\n",
    "\n",
    "\tBuilt-in entities | Description | Example\n",
    "\t--- | --- | ---\n",
    "\tORG | Organization | Apple\n",
    "\tGPE | Geopolitical entity<br>i.e. countries, cities, states | U.K.\n",
    "\tMONEY | Monetary values<br>including unit | $400 million\n",
    "\tDATE | Date | 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('San Francisco', 'GPE')]\n",
      "[('San', 'GPE'), ('Francisco', 'GPE'), ('considers', ''), ('banning', ''), ('sidewalk', ''), ('delivery', ''), ('robots', '')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "\n",
    "# token level\n",
    "ents = [(token.text, token.ent_type_) for token in doc]\n",
    "print(ents) # NOTE: Only first two tkns are named entities in this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EntityRuler\n",
    "- `doc.ents` is writable and can add custom named entities with:\n",
    "\t\tdoc.ents = [Span(doc, text) for text in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher - Pattern Rules\n",
    "- Matchers are beefed up regex to match sequences on tokens\n",
    "- `Matcher` match sequences based on lists of token descriptions\n",
    "- Find words/phrases using rules/patterns describing token attributes\n",
    "\t- Can be token annotations like text, POS tags, lexical attributes\n",
    "- Applying a matcher to a `Doc` gives access to the matched tokens\n",
    "\t- Matched tokens are list[tuples] where (match_id, start_tkn_idx, end_tkn_idx (exclusive))\n",
    "- When writing patterns, one dictionary is one token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matcher Usage Flow\n",
    "1. Initialize Matcher with a vocab\n",
    "2. Add pattern with `matcher.add` method\n",
    "3. Use matcher by calling matcher on a doc\n",
    "4. Returns list[tuples] where (match_id, start, end)\n",
    "\t- match_id is int\n",
    "\t- Use `nlp.vocab.strings[match_id]` to get str version of match_id\n",
    "\t- start, end are token index (where end is exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15578876784678163569 HelloWorld 0 3 Hello, world\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)  # 1. Initialize Matcher with a vocab\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "matcher.add(\"HelloWorld\", [pattern])  # 2. Add pattern to matcher\n",
    "\n",
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)  # 3. Use matcher by calling matcher on a doc\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string rep of match_id\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Matcher\n",
    "- `PhraseMatcher` efficiently match large terminology lists\n",
    "- Accespts match pattersn in the form of `Doc` objects\n",
    "- Same Usage Flow as `Matcher`.\n",
    "\t- Difference is in step 2. Instead of adding pattern list[dict], add list[Doc]\n",
    "- `Doc` pattern can contain single or multiple tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angela Merkel\n",
      "Barack Obama\n",
      "Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
