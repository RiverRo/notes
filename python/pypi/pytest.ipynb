{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytest\n",
    "\n",
    "> [Main Table of Contents](../../README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In This Notebook\n",
    "- Directory structure\n",
    "- Run pytest\n",
    "\t- Identify Node ID\n",
    "- Test structure\n",
    "- Assert Statements\n",
    "\t- Assert with failure message\n",
    "- Pytest Methods\n",
    "- Test for exceptions\n",
    "- Test Setup and Teardown\n",
    "\t- Fixtures\n",
    "\t\t- Create fixtures\n",
    "\t\t- Use fixtures\n",
    "\t\t- Use multiple fixtures - fixture chaining\n",
    "- Mocking\n",
    "- Test Data Science Models\n",
    "- Test Plots\n",
    "- What to test\n",
    "- Pytest report\n",
    "- Continuous Integration and Codecov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pytest\n",
    "- Command line invocation\n",
    "\n",
    "- Find pattern in test function names and test class names and only run if matched substring. Useful for large test collections.\n",
    "\t```python\n",
    "\tpytest -k pattern\n",
    "\t```\n",
    "\n",
    "- Exit immediately on first fail. Useful for large files.\n",
    "\t```python\n",
    "\tpytest -x file_or_dir\n",
    "\t```\n",
    "- Execute specific part of test file or test class with nodeID\n",
    "\t```python\n",
    "\tpytest <path_with_node_id_notation>\n",
    "\t```\n",
    "- Show reason xfail\n",
    "\t```python\n",
    "\tpytest -rx file_or_dir\n",
    "\t```\n",
    "- Show reason for skipped test\n",
    "\t```python\n",
    "\tpytest -rs file_or_dir\n",
    "\t```\n",
    "- Show reason for both skipped and xfailed test\n",
    "\t```python\n",
    "\tpytest -rs file_or_dir\n",
    "\t```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Node ID\n",
    "- Node ID is the last part od the following\n",
    "\t```python\n",
    "\t# Node ID of a test class\n",
    "\t<path_test_module>::<test_class_name>\n",
    "\n",
    "\t# Node ID of unit test\n",
    "\t<path_test_module>::<test_class_name>::<unit_test_name>\n",
    "\t```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory structure\n",
    "- `tests` directory should mirror source code directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test structure\n",
    "- Tests for each functions are organized into a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assert statements\n",
    "- `assert` is main keyword used in unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assert with failure message\n",
    "- Best practice to include message with assert statement which prints on failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a+b\n",
    "\n",
    "class TestAdd():\n",
    "    def test_integers():\n",
    "        expected = 9\n",
    "        actual = add(3, 6)\n",
    "        # Message should include function as called and actual return value\n",
    "        message = f'add(3, 6) should return the int 9, but it actually returned {actual}'\n",
    "        assert actual == expected, message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytest methods\n",
    "\n",
    "Method | Description | Example\n",
    "--- | --- | ---\n",
    "pytest.approx(#) | Handle float datatypes | assert 0.1 + 0.1 + 0.1 == pytest.approx(0.3)<br>np.array([0.1 + 0.1, 0.1 + 0.1]) == pytest.approx([0.2, 0.2])\n",
    "pytest.raises(ErrorType) | Context manager to test for exceptions | The test function is called within this context manager\n",
    "@pytest.mark.xfail(expectedfailreason) | Mark decorated fn as Expected failure | @pytest.mark.xfail(reason='Using TDD, fn not implemented')\n",
    "@pytest.mark.skipif(boolean_expr, reasonskipped) | Mark decorated so pytest skips fn on given condition | @pytest.mark.skipif(sys.version_info > (2, 7), reason='requires Python 2.7')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for exceptions instead of return values\n",
    "- `pytest.raises(ErrorType)` uses a context manager\n",
    "- General structure\n",
    "\t```python\n",
    "\twith pytest.raises(ValueError):   \n",
    "\t\t# Do nothing on entering context  \n",
    "\t\tprint('This is part of the context')  \n",
    "\t\t# if code in this context raises ValueError, good. was expected  \n",
    "\t\t# if code in this context did not raise ValueEror, raise an exception  \n",
    "\t```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test for exception example\n",
    "def add(a, b):\n",
    "    if not isinstance(a, int) or not isinstance(b, int):\n",
    "        raise TypeError('Must pass two integers :)')\n",
    "    return a+b\n",
    "\n",
    "# GOOD\n",
    "def test_typeerror_on_non_int():\n",
    "    #Test to make sure Error is raised\n",
    "    with pytest.raises(TypeError) as exception_info:  \n",
    "        # Doesn't do anything if context raised TypeError\n",
    "        add('3', 5)\n",
    "    # Test for correct message\n",
    "    assert exception_info.match('Must pass two integers :)')\n",
    "\n",
    "# WILL FAIL\n",
    "def test_typeerror_on_non_int():\n",
    "    #Test to make sure Error is raised\n",
    "    with pytest.raises(TypeError) as exception_info:  \n",
    "        print('No code in this context will raise TypeError so will fail')\n",
    "    # Test for correct message\n",
    "    assert exception_info.match('Must pass two integers :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Setup and Teardown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixtures\n",
    "- Fixures run setup code then `yield` or `return` some data/conn/etc then optionally run tear down code\n",
    "- Examples of fixtures:\n",
    "\t-\tSome functions may need access to new blank file in some directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Fixture\n",
    "- Create with `@pytest.fixture`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "@pytest.fixture\n",
    "def new_blank_file():\n",
    "    file = open('new_blank_file.txt', 'w')  # setup\n",
    "    yield file\n",
    "    os.remove(file)  # teardown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Fixture\n",
    "- Use fixture by passing into test function as parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual fn being tested\n",
    "def fn_requires_blnk_file(new_file):\n",
    "    new_file.addedattr = 'Hello new file'\n",
    "    return new_file\n",
    "\n",
    "# Pass fixture in as an argument\n",
    "def test_on_fn_that_requires_new_blank_file(new_blank_file):\n",
    "    actual = fn_requires_blnk_file(new_blank_file).addedattr\n",
    "    expected = 'Hello new file'\n",
    "    assert expected == actual, f'fn_requires_blnk_file(new_blank_file).addedattr should return {expected} but got {actual}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Multiple Fixtures - Fixture chaining\n",
    "- Use fixture in another fixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmpdir is a pytest built-in fixture which setups up tmp dir and tears it down afterwards\n",
    "@pytest.fixture\n",
    "def new_blank_file(tmpdir):\n",
    "    # create new blank file in a temp directory\n",
    "    file_in_tmp_dir = tmpdir.join('new_blank_file.txt')\n",
    "    open(file_in_tmp_dir, 'w').close()\n",
    "    yield file_in_tmp_dir\n",
    "    # teardown not needed b/c tmpdir has tear down code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocking\n",
    "- Test functions independently of their dependencies\n",
    "- Replace a dependency with a bug-free fake (mock)\n",
    "- Use `pytest-mock`\n",
    "\t- *Preferred when using pytest*\n",
    "\t- Undoes the mocking automatically after the end of the test\n",
    "\t- Provides other nice utilities such as spy and stub\n",
    "\t- Uses pytest introspection when comparing calls\n",
    "- Use `unittest.mock.MagicMock`\n",
    "\t- MagicMock includes magic methods\n",
    "\t- alternative mocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytest_mock.mocker.patch() example\n",
    "\n",
    "# preprocess() takes a raw file and clean file path.  \n",
    "#   Inside preprocess, it runs two functions on the raw file \n",
    "#   then writes the clean data to clean file.\n",
    "#   The two dependencies are: row_to_list() and convert_to_int()\n",
    "# I want to test preprocess indenpendently of the two dependecies\n",
    "# Two bug_free mock functions will replace the real dependencies\n",
    "\n",
    "# These functions in: data.preprocessing_helpers directory\n",
    "def row_to_list(row_str):\n",
    "    # buggy fn\n",
    "    return row_str.split('addbug')\n",
    "\n",
    "def convert_to_int(num_str):\n",
    "    # buggy fn\n",
    "    return int(num_str, 'addbug')\n",
    "\n",
    "def preprocess(raw, clean):\n",
    "    raw_read = open(raw)\n",
    "    semi_cleaned = row_to_list(raw_read)   # dependency\n",
    "    cleaned = convert_to_int(semi_cleaned) # dependency\n",
    "    open(clean, 'w').write(cleaned)\n",
    "\n",
    "# Test functions\n",
    "from unittest.mock import call\n",
    "from pytest_mock import mocker\n",
    "# Define two bug free versions of dependencies.\n",
    "# Now if the test fails, it's b/c bug in preprocess not bug in dependency\n",
    "def row_to_list_bug_free(row_str):\n",
    "    return_values = {\"1,801\\t201,411\\n\": [\"1,801\", \"201,411\"],\n",
    "                     \"1,767565,112\\n\": None,\n",
    "                     \"2,002\\t333,209\\n\": [\"2,002\", \"333,209\"],\n",
    "                     \"1990\\t782,911\\n\": [\"1990\", \"782,911\"],\n",
    "                     \"1,285\\t389129\\n\": [\"1,285\", \"389129\"]}\n",
    "    return return_values[row_str]\n",
    "\n",
    "def convert_to_int_bug_free(num_str):\n",
    "    return_values = {\"1,801\": 1801, \"201,411\": 201411, \"2,002\": 2002, \"333,209\": 333209, \"1990\": None, \"782,911\": 782911, \"1,285\": 1285, \"389129\": None}\n",
    "    return return_values[num_str]\n",
    "#-------------------------------------------------------------\n",
    "# Finally the actual test function\n",
    "def test_on_raw_data(raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file  # fixture produces file paths\n",
    "    # Replace the dependency with the bug-free mock\n",
    "    # First arg is name of the path to real function\n",
    "    # The side effect will be called to get faked bug-free values\n",
    "    row_to_list_mock = mocker.patch('data.preprocessing_helpers.row_to_list')\n",
    "    row_to_list_mock.side_effect = row_to_list_bug_free  # alt way to add side_effect\n",
    "    # Replace the dependency with the bug-free mock\n",
    "    # First arg is name of the path to real function\n",
    "    # The side effect will be called to get faked bug-free values\n",
    "    convert_to_int_mock = mocker.patch('data.preprocessing_helpers.convert_to_int', \n",
    "                                       side_effect=convert_to_int_bug_free) \n",
    "    preprocess(raw_path, clean_path)\n",
    "    # How to tell if the bug_free versions were called instead of the actual fns?\n",
    "    # Check if preprocess() called the dependencies correctly by checking call_args_list\n",
    "    assert row_to_list_mock.call_args_list == [call(\"1,801\\t201,411\\n\"), call(\"1,767565,112\\n\"), call(\"2,002\\t333,209\\n\"), call(\"1990\\t782,911\\n\"), call(\"1,285\\t389129\\n\") ]\n",
    "\n",
    "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
    "\n",
    "    # Do additional checks on the clean file data\n",
    "    with open(clean_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\\\t333209\\\\n\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Data Science Models\n",
    "- Difficult b/c expected return values are difficult to manually compute\n",
    "- Often utilizes `r-squared` which indicates how well the model performs on unseen data\n",
    "- 0 <=`r-squared`<=1\n",
    "- `r-squared` == 1 indicates perfect fit\n",
    "\n",
    "\tTypes of data science models | Possible solution\n",
    "\t--- | ---\n",
    "\tLinear regression | Use an exact line\n",
    "\tRandom forest | Use inequalities and equalities<br>Set Expected `r-squared`== 0\n",
    "\tSupport Vector Machines | Use inequalities and equalities<br>Set Expected `r-squared`== 0\n",
    "\tNeural Networks | Use inequalities and equalities<br>Set Expected `r-squared`== 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing plots [(DataCamp)](https://campus.datacamp.com/courses/unit-testing-for-data-science-in-python/testing-models-plots-and-much-more?ex=11)\n",
    "- Test plotting functions that return matplotlib figure(s)\n",
    "- Use `pytest-mpl`\n",
    "\t- Ignores OS related differences\n",
    "\t- Makes it easy to generate baseline images to test against\n",
    "\t- Produces a layered image to show where the differences are, if any  \n",
    "\n",
    "1. Generate one-time baseline plot\n",
    "\t- Decide on test arguments which is a custom collection of mpl attributes\n",
    "\t- Call plotting function on test arguments\n",
    "\t- Convert `Figure()` to PNG image\n",
    "\t- Image looks OK?\n",
    "\t- If yes, sotre images as baseline\n",
    "\t- If no, fix plotting function\n",
    "\t\n",
    "2. Test against the baseline plot\n",
    "\t- Call plotting function on test arguments\n",
    "\t- Convert `Figure()` to PNG image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to test  ( TODO: comeback to this )\n",
    "- Should test 1-2 versions of each category\n",
    "- Not all functions have bad or special arguments\n",
    "- Test categories\n",
    "\t- Bad\n",
    "\t\t- See `Testing for exceptions` section\n",
    "\t- Special\n",
    "\t\t- Boundary values\n",
    "\t\t- Argument values\n",
    "\t- Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest Report\n",
    "- A period indicates test passed\n",
    "- 'F' indicates test failed\n",
    "\t- typically due to raised error\n",
    "\t- line containing 'where' displays actual return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Integration and Codecov\n",
    "- Look into CircleCI\n",
    "- [TravisCI](./travisci.ipynb) with Github integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Coverage\n",
    "- Codecov [(via TravicCI)](./travisci.ipynb) avail with Github integration\n",
    "- [Code Climate](https://codeclimate.com/quality)\n",
    "\t- Automated code review comments on pull requests\n",
    "\t- Never merge code without sufficient tests\n",
    "\t- Identify hot sppots to focus on what matters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
